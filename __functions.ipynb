{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate_customer_profiles_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_customer_profiles_table(n_customers, random_state=42):\n",
    "    \n",
    "    np.random.seed(random_state)\n",
    "        \n",
    "    customer_id_properties = []\n",
    "    \n",
    "    # Generate customer properties from random distributions \n",
    "    for customer_id in range(n_customers):\n",
    "        \n",
    "        x_customer_id = np.random.uniform(0,100)\n",
    "        y_customer_id = np.random.uniform(0,100)\n",
    "        \n",
    "        mean_amount = np.random.uniform(5,100) # Arbitrary (but sensible) value \n",
    "        std_amount = mean_amount/2 # Arbitrary (but sensible) value\n",
    "        \n",
    "        mean_nb_tx_per_day = np.random.uniform(0,4) # Arbitrary (but sensible) value \n",
    "        \n",
    "        customer_id_properties.append([customer_id,\n",
    "                                      x_customer_id, y_customer_id,\n",
    "                                      mean_amount, std_amount,\n",
    "                                      mean_nb_tx_per_day])\n",
    "        \n",
    "    customer_profiles_table = pd.DataFrame(\n",
    "        customer_id_properties,\n",
    "        columns=[\n",
    "            'customer_id',\n",
    "            'x_customer_id', 'y_customer_id',\n",
    "            'mean_amount', 'std_amount',\n",
    "            'mean_nb_tx_per_day'\n",
    "        ])\n",
    "    \n",
    "    return customer_profiles_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate_terminal_profiles_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_terminal_profiles_table(n_terminals, random_state=42):\n",
    "    \n",
    "    np.random.seed(random_state)\n",
    "        \n",
    "    terminal_id_properties = []\n",
    "    \n",
    "    # Generate terminal properties from random distributions \n",
    "    for terminal_id in range(n_terminals):\n",
    "        \n",
    "        x_terminal_id = np.random.uniform(0,100)\n",
    "        y_terminal_id = np.random.uniform(0,100)\n",
    "        \n",
    "        terminal_id_properties.append([\n",
    "            terminal_id,\n",
    "            x_terminal_id, y_terminal_id\n",
    "        ])\n",
    "                                       \n",
    "    terminal_profiles_table = pd.DataFrame(\n",
    "        terminal_id_properties,\n",
    "        columns=['terminal_id', 'x_terminal_id', 'y_terminal_id']\n",
    "    )\n",
    "    \n",
    "    return terminal_profiles_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_list_terminals_within_radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_terminals_within_radius(customer_profile, x_y_terminals, r):\n",
    "    \n",
    "    # Use numpy arrays in the following to speed up computations\n",
    "    \n",
    "    # Location (x,y) of customer as numpy array\n",
    "    x_y_customer = \\\n",
    "        customer_profile[['x_customer_id','y_customer_id']].values.astype(float)\n",
    "    \n",
    "    # Squared difference in coordinates between customer and terminal locations\n",
    "    squared_diff_x_y = np.square(x_y_customer - x_y_terminals)\n",
    "    \n",
    "    # Sum along rows and compute suared root to get distance\n",
    "    dist_x_y = np.sqrt(np.sum(squared_diff_x_y, axis=1))\n",
    "    \n",
    "    # Get the indices of terminals which are at a distance less than r\n",
    "    available_terminals = list(np.where(dist_x_y < r)[0])\n",
    "    \n",
    "    # Return the list of terminal IDs\n",
    "    return available_terminals\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate_transactions_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_transactions_table(customer_profile, nb_days=10):\n",
    "    \n",
    "    customer_transactions = []\n",
    "    \n",
    "    random.seed(int(customer_profile['customer_id']))\n",
    "    np.random.seed(int(customer_profile['customer_id']))\n",
    "    \n",
    "    # For all days\n",
    "    for day in range(nb_days):\n",
    "        \n",
    "        # Random number of transactions for that day \n",
    "        nb_tx = np.random.poisson(customer_profile.mean_nb_tx_per_day)\n",
    "        \n",
    "        # If nb_tx positive, let us generate transactions\n",
    "        if nb_tx > 0:\n",
    "            \n",
    "            for tx in range(nb_tx):\n",
    "                \n",
    "                # Time of transaction: Around noon, std 20000 seconds. \n",
    "                # This choice aims at simulating the fact that \n",
    "                # most transactions occur during the day.\n",
    "                time_tx = int(np.random.normal(86400 / 2, 20000))\n",
    "                \n",
    "                # If transaction time between 0 and 86400, \n",
    "                # let us keep it, otherwise, let us discard it\n",
    "                if (time_tx > 0) and (time_tx < 86400):\n",
    "                    \n",
    "                    # Amount is drawn from a normal distribution  \n",
    "                    amount = np.random.normal(\n",
    "                        customer_profile.mean_amount, \n",
    "                        customer_profile.std_amount\n",
    "                    )\n",
    "                    \n",
    "                    # If amount negative, draw from a uniform distribution\n",
    "                    if amount < 0:\n",
    "                        \n",
    "                        amount = np.random.uniform(\n",
    "                            0, customer_profile.mean_amount*2\n",
    "                        )\n",
    "                    \n",
    "                    amount = np.round(amount, decimals=2)\n",
    "                    \n",
    "                    if len(customer_profile.available_terminals) > 0:\n",
    "                        \n",
    "                        terminal_id = random.choice(\n",
    "                            customer_profile.available_terminals\n",
    "                        )\n",
    "                    \n",
    "                        customer_transactions.append([\n",
    "                            time_tx + day * 86400,\n",
    "                            day,\n",
    "                            customer_profile['customer_id'],\n",
    "                            terminal_id,\n",
    "                            amount\n",
    "                        ])\n",
    "            \n",
    "    customer_transactions = pd.DataFrame(\n",
    "        customer_transactions,\n",
    "        columns=[\n",
    "            'tx_time_seconds', 'tx_time_days',\n",
    "            'customer_id', 'terminal_id', 'tx_amount'\n",
    "        ])\n",
    "    \n",
    "    if len(customer_transactions) > 0:\n",
    "        \n",
    "        customer_transactions['tx_datetime'] = \\\n",
    "            pd.to_datetime(\n",
    "                customer_transactions[\"tx_time_seconds\"],\n",
    "                unit='s', origin=start_date\n",
    "            )\n",
    "\n",
    "        customer_transactions = \\\n",
    "            customer_transactions[[\n",
    "                'tx_datetime','customer_id', 'terminal_id',\n",
    "                'tx_amount','tx_time_seconds', 'tx_time_days'\n",
    "            ]]\n",
    "    \n",
    "    return customer_transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add_frauds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_frauds(customer_profiles_table, terminal_profiles_table, data):\n",
    "    \n",
    "    # By default, all transactions are genuine\n",
    "    data['tx_fraud'] = 0\n",
    "    data['tx_fraud_scenario'] = 0\n",
    "    \n",
    "    # Scenario 1\n",
    "    data.loc[data['tx_amount'] > 220, 'tx_fraud'] = 1\n",
    "    data.loc[data['tx_amount'] > 220, 'tx_fraud_scenario'] = 1\n",
    "    nb_frauds_scenario_1 = data['tx_fraud'].sum()\n",
    "    \n",
    "    # Scenario 2\n",
    "    for day in range(data['tx_time_days'].max()):\n",
    "        \n",
    "        compromised_terminals = \\\n",
    "            terminal_profiles_table['terminal_id'].sample(n=2,\n",
    "                                                          random_state=day)\n",
    "        \n",
    "        compromised_transactions = data[\n",
    "            (data['tx_time_days'] >= day) &\n",
    "            (data['tx_time_days'] < day + 28) &\n",
    "            (data['terminal_id'].isin(compromised_terminals))\n",
    "        ]\n",
    "                            \n",
    "        data.loc[compromised_transactions.index, 'tx_fraud'] = 1\n",
    "        data.loc[compromised_transactions.index, 'tx_fraud_scenario'] = 2\n",
    "    \n",
    "    nb_frauds_scenario_2 = data['tx_fraud'].sum() - nb_frauds_scenario_1\n",
    "    \n",
    "    # Scenario 3\n",
    "    for day in range(data['tx_time_days'].max()):\n",
    "        \n",
    "        compromised_customers = \\\n",
    "            customer_profiles_table['customer_id'].sample(n=3, random_state=day) \\\n",
    "                                                    .values\n",
    "        \n",
    "        compromised_transactions = data[\n",
    "            (data['tx_time_days'] >= day) &\n",
    "            (data['tx_time_days'] < day + 14) &\n",
    "            (data['customer_id'].isin(compromised_customers))\n",
    "        ]\n",
    "        \n",
    "        nb_compromised_transactions = len(compromised_transactions)\n",
    "        \n",
    "        random.seed(day)\n",
    "        \n",
    "        index_fauds = random.sample(\n",
    "            list(compromised_transactions.index.values), \n",
    "            k=int(nb_compromised_transactions / 3)\n",
    "        )\n",
    "        \n",
    "        data.loc[index_fauds, 'tx_amount'] = \\\n",
    "            data.loc[index_fauds,'tx_amount'] * 5\n",
    "        \n",
    "        data.loc[index_fauds, 'tx_fraud'] = 1\n",
    "        data.loc[index_fauds, 'tx_fraud_scenario'] = 3\n",
    "        \n",
    "                             \n",
    "    nb_frauds_scenario_3 = \\\n",
    "        data['tx_fraud'].sum() - nb_frauds_scenario_2 - nb_frauds_scenario_1\n",
    "    \n",
    "    return data                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(\n",
    "        start_date, random_state=42, n_customers=10000,\n",
    "        n_terminals=1000000, nb_days=90, r=5):\n",
    "    \n",
    "    customer_profiles_table = \\\n",
    "        generate_customer_profiles_table(n_customers,\n",
    "                                         random_state=random_state)\n",
    "    \n",
    "    terminal_profiles_table = \\\n",
    "        generate_terminal_profiles_table(n_terminals,\n",
    "                                         random_state=random_state+1)\n",
    "    \n",
    "    x_y_terminals = \\\n",
    "        (terminal_profiles_table[['x_terminal_id','y_terminal_id']]\n",
    "         .values\n",
    "         .astype(float))\n",
    "\n",
    "    customer_profiles_table['available_terminals'] = \\\n",
    "        (customer_profiles_table\n",
    "         .apply(lambda x : get_list_terminals_within_radius(\n",
    "                              x,\n",
    "                              x_y_terminals=x_y_terminals,\n",
    "                              r=r\n",
    "                           ),\n",
    "                axis=1))\n",
    "            \n",
    "    customer_profiles_table['nb_terminals'] = \\\n",
    "        customer_profiles_table.available_terminals.apply(len)\n",
    "            \n",
    "    df = (customer_profiles_table\n",
    "          .groupby('customer_id')\n",
    "          .apply(lambda x : generate_transactions_table(x.iloc[0], nb_days))\n",
    "          .reset_index(drop=True))\n",
    "    \n",
    "    # Sort transactions chronologically\n",
    "    df = df.sort_values('tx_datetime')\n",
    "    # Reset indices, starting from 0\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    df.reset_index(inplace=True)\n",
    "    df.rename(columns={'index':'transaction_id'}, inplace=True)\n",
    "    \n",
    "    return customer_profiles_table, terminal_profiles_table, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read_from_files\n",
    "\n",
    "First use in [Chapter 3, Baseline Feature Transformation](Baseline_Feature_Transformation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a set of pickle files, put them together in a single DataFrame, and order them by time\n",
    "# It takes as input the folder DIR_INPUT where the files are stored, and the BEGIN_DATE and END_DATE\n",
    "def read_from_files(DIR_INPUT, BEGIN_DATE, END_DATE, sort='transaction_id'):\n",
    "    \n",
    "    files = [os.path.join(DIR_INPUT, f) for f in os.listdir(DIR_INPUT) \\\n",
    "             if f >= BEGIN_DATE + '.pkl' and f <= END_DATE + '.pkl']\n",
    "\n",
    "    frames = []\n",
    "    \n",
    "    for f in files:\n",
    "        df = pd.read_pickle(f)\n",
    "        frames.append(df)\n",
    "        del df\n",
    "        \n",
    "    df_final = pd.concat(frames)\n",
    "    \n",
    "    df_final = df_final.sort_values(sort)\n",
    "    df_final.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    #  Note: -1 are missing values for real world data \n",
    "    df_final = df_final.replace([-1],0)\n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save_object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save oject as pickle file\n",
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as output:\n",
    "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### is_weekend, is_night"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_weekend(tx_datetime):\n",
    "    \n",
    "    # Transform date into weekday (0 is Monday, 6 is Sunday)\n",
    "    weekday = tx_datetime.weekday()\n",
    "    # Binary value: 0 if weekday, 1 if weekend\n",
    "    is_weekend = weekday>=5\n",
    "    \n",
    "    return int(is_weekend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_night(tx_datetime):\n",
    "    \n",
    "    # Get the hour of the transaction\n",
    "    tx_hour = tx_datetime.hour\n",
    "    # Binary value: 1 if hour less than 6, and 0 otherwise\n",
    "    is_night = tx_hour<=6\n",
    "    \n",
    "    return int(is_night)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_customer_spending_behaviour_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_customer_spending_behaviour_features(customer_transactions,\n",
    "                                             windows_size_in_days=[1,7,30]):\n",
    "    \n",
    "    # Let us first order transactions chronologically\n",
    "    customer_transactions = customer_transactions.sort_values('tx_datetime')\n",
    "    \n",
    "    # The transaction date and time is set as the index, \n",
    "    # which will allow the use of the rolling function \n",
    "    customer_transactions.index = customer_transactions['tx_datetime']\n",
    "    \n",
    "    # For each window size\n",
    "    for window_size in windows_size_in_days:\n",
    "        \n",
    "        # Compute the sum of the transaction amounts and \n",
    "        # the number of transactions for the given window size\n",
    "        SUM_AMOUNT_TX_WINDOW = customer_transactions['tx_amount'] \\\n",
    "                                   .rolling(str(window_size) + 'd').sum()\n",
    "        \n",
    "        NB_TX_WINDOW = customer_transactions['tx_amount'] \\\n",
    "                           .rolling(str(window_size) + 'd').count()\n",
    "    \n",
    "        # Compute the average transaction amount for the given window size\n",
    "        # NB_TX_WINDOW is always >0 since current transaction is always included\n",
    "        AVG_AMOUNT_TX_WINDOW = SUM_AMOUNT_TX_WINDOW / NB_TX_WINDOW\n",
    "    \n",
    "        # Save feature values\n",
    "        customer_nb_name = 'customer_id_nb_tx_' + str(window_size) + 'day_window'\n",
    "        customer_avg_name = 'customer_id_avg_amount_' + str(window_size) + 'day_window'\n",
    "        \n",
    "        customer_transactions[customer_nb_name] = list(NB_TX_WINDOW)\n",
    "        customer_transactions[customer_avg_name] = list(AVG_AMOUNT_TX_WINDOW)\n",
    "    \n",
    "    # Reindex according to transaction IDs\n",
    "    customer_transactions.index = customer_transactions['transaction_id']\n",
    "        \n",
    "    # And return the dataframe with the new features\n",
    "    return customer_transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_count_risk_rolling_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count_risk_rolling_window(terminal_transactions,\n",
    "                                  delay_period=7,\n",
    "                                  windows_size_in_days=[1,7,30],\n",
    "                                  feature='terminal_id'):\n",
    "    \n",
    "    terminal_transactions = terminal_transactions.sort_values('tx_datetime')\n",
    "    \n",
    "    terminal_transactions.index = terminal_transactions['tx_datetime']\n",
    "\n",
    "    name_delay = str(delay_period) + 'd'\n",
    "    \n",
    "    nb_fraud_delay = terminal_transactions['tx_fraud'].rolling(name_delay).sum()\n",
    "    nb_tx_delay = terminal_transactions['tx_fraud'].rolling(name_delay).count()\n",
    "    \n",
    "    for window_size in windows_size_in_days:\n",
    "\n",
    "        name_window = str(delay_period+window_size) + 'd'\n",
    "        \n",
    "        nb_fraud_delay_window = \\\n",
    "            terminal_transactions['tx_fraud'].rolling(name_window).sum()\n",
    "        nb_tx_delay_window = \\\n",
    "            terminal_transactions['tx_fraud'].rolling(name_window).count()\n",
    "    \n",
    "        nb_fraud_window = nb_fraud_delay_window - nb_fraud_delay\n",
    "        nb_tx_window = nb_tx_delay_window - nb_tx_delay\n",
    "        risk_window = nb_fraud_window / nb_tx_window\n",
    "\n",
    "        name_nb = feature + '_nb_tx_' + str(window_size) + 'day_window'\n",
    "        name_risk = feature + '_risk_' + str(window_size) + 'day_window'\n",
    "        \n",
    "        terminal_transactions[name_nb] = list(nb_tx_window)\n",
    "        terminal_transactions[name_risk] = list(risk_window)\n",
    "        \n",
    "    terminal_transactions.index = terminal_transactions['transaction_id']\n",
    "    \n",
    "    # Replace NA values with 0 (all undefined risk scores where NB_TX_WINDOW is 0) \n",
    "    terminal_transactions.fillna(0, inplace=True)\n",
    "    \n",
    "    return terminal_transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_train_test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_set(\n",
    "        data, start_date_training, delta_train=7, \n",
    "        delta_delay=7, delta_test=7, sampling_ratio=1.0, random_state=42):\n",
    "\n",
    "    end_date_training = (start_date_training\n",
    "                        + dt.timedelta(days=delta_train))\n",
    "    \n",
    "    # Get the training set data\n",
    "    train_df = data[\n",
    "            (data['tx_datetime'] >= start_date_training) \n",
    "            & (data['tx_datetime'] < end_date_training)].copy()\n",
    "    \n",
    "    # Get the test set data\n",
    "    test_df = []\n",
    "    \n",
    "    # Note: Cards known to be compromised after \n",
    "    # the delay period are removed from the test set\n",
    "    # That is, for each test day, all frauds known at (test_day-delay_period) \n",
    "    # are removed\n",
    "    \n",
    "    # First, get known defrauded customers from the training set\n",
    "    known_defrauded_customers = \\\n",
    "        set(train_df[train_df['tx_fraud']==1]['customer_id'])\n",
    "    \n",
    "    # Get the relative starting day of training set \n",
    "    # (easier than TX_DATETIME to collect test data)\n",
    "    start_tx_time_days_training = train_df['tx_time_days'].min()\n",
    "    \n",
    "            \n",
    "    # Then, for each day of the test set\n",
    "    for day in range(delta_test):\n",
    "\n",
    "        # current test day number since the start of training\n",
    "        test_day = sum([\n",
    "            start_tx_time_days_training,\n",
    "            delta_train,\n",
    "            delta_delay,\n",
    "            day\n",
    "        ])\n",
    "        \n",
    "        # Get whole test data for that day\n",
    "        test_day_df = data[data['tx_time_days']==test_day]\n",
    "        \n",
    "        # find current day minus delay period\n",
    "        test_day_delay = test_day - delta_delay\n",
    "        \n",
    "        # Compromised cards from that test day minus the delay period, \n",
    "        # are added to the pool of known defrauded customers\n",
    "        test_day_df_delay = data[data['tx_time_days']==test_day_delay-1].copy()\n",
    "\n",
    "        new_defrauded_customers = \\\n",
    "            set(test_day_df_delay[test_day_df_delay['tx_fraud']==1]['customer_id'])\n",
    "\n",
    "        # add defrauded customers from delay period to known defrauded customers\n",
    "        known_defrauded_customers = (known_defrauded_customers\n",
    "                                     .union(new_defrauded_customers))\n",
    "\n",
    "        # remove from cureent test day data all known customers at that moment:\n",
    "        # first, all defrauded customers from train dataset\n",
    "        # second, all defrauded customers from start of delay period till \n",
    "        # current test day minus delay period and minus one day \n",
    "        # (because we can't get all frauds of current test day minus \n",
    "        # delay period immediately - so, for current test day we get \n",
    "        # data from \"yesterday minus delay period\")\n",
    "        test_day_df = (test_day_df[~test_day_df['customer_id']\n",
    "                       .isin(known_defrauded_customers)])\n",
    "        \n",
    "        test_df.append(test_day_df)\n",
    "        \n",
    "    test_df = pd.concat(test_df)\n",
    "    \n",
    "    # If subsample\n",
    "    if sampling_ratio < 1:\n",
    "        \n",
    "        train_df_frauds = (train_df[train_df['tx_fraud'] == 1]\n",
    "                           .sample(frac=sampling_ratio,\n",
    "                                   random_state=random_state))\n",
    "        \n",
    "        train_df_genuine = (train_df[train_df['tx_fraud'] == 0]\n",
    "                            .sample(frac=sampling_ratio,\n",
    "                                    random_state=random_state))\n",
    "        \n",
    "        train_df = pd.concat([train_df_frauds, train_df_genuine])\n",
    "        \n",
    "    # Sort data sets by ascending order of transaction ID\n",
    "    train_df = train_df.sort_values('transaction_id')\n",
    "    test_df = test_df.sort_values('transaction_id')\n",
    "    \n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prequentialSplit\n",
    "\n",
    "First use in [Chapter 5, Validation Strategies](Validation_Strategies)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prequentialSplit(\n",
    "        transactions_df, start_date_training, n_folds=4,\n",
    "        delta_train=7, delta_delay=7, delta_assessment=7):\n",
    "    \n",
    "    prequential_split_indices=[]\n",
    "        \n",
    "    # For each fold\n",
    "    for fold in range(n_folds):\n",
    "        \n",
    "        # Shift back start date for training by the fold index times\n",
    "        # the assessment period (delta_assessment)\n",
    "        start_date_training_fold = (start_date_training \n",
    "                                    - dt.timedelta(days=fold*delta_assessment))\n",
    "        \n",
    "        # Get the training and test (assessment) sets\n",
    "        (train_df, test_df) = get_train_test_set(transactions_df,\n",
    "                                                 start_date_training_fold,\n",
    "                                                 delta_train=delta_train,\n",
    "                                                 delta_delay=delta_delay,\n",
    "                                                 delta_test=delta_assessment)\n",
    "    \n",
    "        # Get the indices from the two sets, and add them to the list of prequential splits\n",
    "        indices_train=list(train_df.index)\n",
    "        indices_test=list(test_df.index)\n",
    "        \n",
    "        prequential_split_indices.append((indices_train,indices_test))\n",
    "    \n",
    "    return prequential_split_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### card_precision_top_k_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def card_precision_top_k_day(df_day, top_k):\n",
    "    \n",
    "    # This takes the max of the predictions AND the max of label TX_FRAUD for each CUSTOMER_ID, \n",
    "    # and sorts by decreasing order of fraudulent prediction\n",
    "    df_day = (df_day\n",
    "              .groupby('customer_id')\n",
    "              .max()\n",
    "              .sort_values(by=\"predictions\", ascending=False)\n",
    "              .reset_index(drop=False))\n",
    "            \n",
    "    # Get the top k most suspicious cards\n",
    "    df_day_top_k = df_day.head(top_k)\n",
    "    \n",
    "    list_detected_compromised_cards = \\\n",
    "        list(df_day_top_k[df_day_top_k['tx_fraud']==1]['customer_id'])\n",
    "    \n",
    "    # Compute precision top k\n",
    "    card_precision_top_k = len(list_detected_compromised_cards) / top_k\n",
    "    \n",
    "    return list_detected_compromised_cards, card_precision_top_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### card_precision_top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def card_precision_top_k(\n",
    "        predictions_df, top_k,\n",
    "        remove_detected_compromised_cards=True):\n",
    "\n",
    "    # Sort days by increasing order\n",
    "    list_days=list(predictions_df['tx_time_days'].unique())\n",
    "    list_days.sort()\n",
    "    \n",
    "    # At first, the list of detected compromised cards is empty\n",
    "    list_detected_compromised_cards = []\n",
    "    \n",
    "    card_precision_top_k_per_day_list = []\n",
    "    nb_compromised_cards_per_day = []\n",
    "    \n",
    "    # For each day, compute precision top k\n",
    "    for day in list_days:\n",
    "        \n",
    "        df_day = predictions_df[predictions_df['tx_time_days']==day]\n",
    "        df_day = df_day[['predictions', 'customer_id', 'tx_fraud']]\n",
    "        \n",
    "        # Let us remove detected compromised cards from the set of daily transactions\n",
    "        df_day = (df_day[df_day['customer_id']\n",
    "                  .isin(list_detected_compromised_cards)==False])\n",
    "\n",
    "        num_cards = len(df_day[df_day['tx_fraud']==1]['customer_id'].unique())\n",
    "        nb_compromised_cards_per_day.append(num_cards)\n",
    "        \n",
    "        (detected_compromised_cards,\n",
    "         card_precision_top_k) = card_precision_top_k_day(df_day,top_k)\n",
    "        \n",
    "        card_precision_top_k_per_day_list.append(card_precision_top_k)\n",
    "        \n",
    "        # Let us update the list of detected compromised cards\n",
    "        if remove_detected_compromised_cards:\n",
    "            list_detected_compromised_cards.extend(detected_compromised_cards)\n",
    "        \n",
    "    # Compute the mean\n",
    "    mean_card_precision_top_k = np.array(card_precision_top_k_per_day_list).mean()\n",
    "    \n",
    "    # Returns precision top k per day as a list, and resulting mean\n",
    "    return (nb_compromised_cards_per_day,\n",
    "            card_precision_top_k_per_day_list,\n",
    "            mean_card_precision_top_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### card_precision_top_k_custom\n",
    "\n",
    "First use in [Chapter 5, Validation Strategies](Validation_Strategies)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def card_precision_top_k_custom(y_true, y_pred, top_k, data):\n",
    "    \n",
    "    # Let us create a predictions_df DataFrame, that contains all transactions matching the indices of the current fold\n",
    "    # (indices of the y_true vector)\n",
    "    predictions_df = data.iloc[y_true.index.values].copy()\n",
    "    predictions_df['predictions'] = y_pred\n",
    "    \n",
    "    # Compute the CP@k using the function implemented in Chapter 4, Section 4.2\n",
    "    (nb_compromised_cards_per_day,\n",
    "     card_precision_top_k_per_day_list,\n",
    "     mean_card_precision_top_k) = card_precision_top_k(predictions_df, top_k)\n",
    "    \n",
    "    # Return the mean_card_precision_top_k\n",
    "    return mean_card_precision_top_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prequential_grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prequential_grid_search(\n",
    "        transactions_df, classifier, parameters, features, target,\n",
    "        scoring, start_date_training, subset='Test', preprop_list=[],\n",
    "        n_folds=4, delta_train=7, delta_delay=7, delta_assessment=7,\n",
    "        performance_metrics_list_grid=['roc_auc'], performance_metrics_list=['AUC ROC'],\n",
    "        search_type='grid', n_iter=None, random_state=42, n_jobs=-1):\n",
    "\n",
    "    estimators = preprop_list.copy()\n",
    "    estimators.extend([('clf', classifier)])\n",
    "                                \n",
    "    pipe = Pipeline(estimators)\n",
    "    \n",
    "    prequential_split_indices = prequentialSplit(\n",
    "                                    transactions_df,\n",
    "                                    start_date_training=start_date_training,\n",
    "                                    n_folds=n_folds, \n",
    "                                    delta_train=delta_train, \n",
    "                                    delta_delay=delta_delay, \n",
    "                                    delta_assessment=delta_assessment\n",
    "                                )\n",
    "    \n",
    "    if search_type==\"grid\":\n",
    "\n",
    "        cv = GridSearchCV(pipe,\n",
    "                          parameters,\n",
    "                          scoring=scoring,\n",
    "                          cv=prequential_split_indices,\n",
    "                          refit=False,\n",
    "                          n_jobs=n_jobs)\n",
    "\n",
    "    if search_type==\"random\":\n",
    "        \n",
    "        cv = RandomizedSearchCV(pipe,\n",
    "                                parameters,\n",
    "                                scoring=scoring,\n",
    "                                cv=prequential_split_indices,\n",
    "                                refit=False,\n",
    "                                n_jobs=n_jobs,\n",
    "                                n_iter=n_iter,\n",
    "                                random_state=random_state)\n",
    "\n",
    "    \n",
    "    X = transactions_df[features]\n",
    "    y = transactions_df[target]\n",
    "\n",
    "    cv.fit(X, y)\n",
    "\n",
    "    performances_df = pd.DataFrame()\n",
    "    \n",
    "    for i in range(len(performance_metrics_list_grid)):\n",
    "\n",
    "        metric_col = performance_metrics_list[i] + ' ' + subset\n",
    "        metric_col_std = performance_metrics_list[i] + ' ' + subset + ' Std'\n",
    "        \n",
    "        metric_grid = 'mean_test_' + performance_metrics_list_grid[i]\n",
    "        metric_grid_std = 'std_test_' + performance_metrics_list_grid[i]\n",
    "        \n",
    "        performances_df[metric_col] = cv.cv_results_[metric_grid]\n",
    "        performances_df[metric_col_std] = cv.cv_results_[metric_grid_std]\n",
    "\n",
    "    performances_df['Parameters'] = cv.cv_results_['params']\n",
    "    performances_df['Fit time'] = cv.cv_results_['mean_fit_time']\n",
    "    performances_df['Score time'] = cv.cv_results_['mean_score_time']\n",
    "    \n",
    "    return performances_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grid_create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_create(\n",
    "        clf, params, preprop_list, search_type='grid', n_iter=None,\n",
    "        random_state=42, key_params_idxs=0, time_exec=False):\n",
    "    \n",
    "    grid = {\n",
    "        # 'data': data,\n",
    "        'clf': clf,\n",
    "        'params': params,\n",
    "        'features': features,\n",
    "        'target': target,\n",
    "        'scoring': scoring,\n",
    "        'train_start_valid': train_start_valid,\n",
    "        'train_start_test': train_start_test,\n",
    "        'preprop_list': preprop_list,\n",
    "        'n_folds': n_folds,\n",
    "        'delta_train': delta_train,\n",
    "        'delta_delay': delta_delay,\n",
    "        'delta_assessment': delta_assessment,\n",
    "        'metrics_list_grid': metrics_grid,\n",
    "        'metrics_list': metrics,\n",
    "        'search_type': search_type,\n",
    "        'n_jobs': n_jobs,\n",
    "        'n_iter': n_iter,\n",
    "        'random_state': random_state,\n",
    "        'key_params_idxs': key_params_idxs,\n",
    "        'time_exec': time_exec\n",
    "    }\n",
    "\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model_performance_CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_performance_CV(\n",
    "        data, clf, params, features, target,\n",
    "        preprop_list, scoring, train_start_valid, train_start_test,\n",
    "        n_folds, delta_train, delta_delay, delta_assessment,\n",
    "        metrics_list_grid, metrics_list, search_type, n_jobs,\n",
    "        n_iter=None, random_state=42, key_params_idxs=0, time_exec=True):\n",
    "\n",
    "    t_start = time.time()\n",
    "    \n",
    "    performance_validation = prequential_grid_search(\n",
    "        data,\n",
    "        clf,\n",
    "        params,\n",
    "        features,\n",
    "        target,\n",
    "        scoring,\n",
    "        start_date_training=train_start_valid,\n",
    "        preprop_list=preprop_list,\n",
    "        n_folds=n_folds,\n",
    "        subset='Validation',\n",
    "        delta_train=delta_train,\n",
    "        delta_delay=delta_delay,\n",
    "        delta_assessment=delta_assessment,\n",
    "        performance_metrics_list_grid=metrics_list_grid,\n",
    "        performance_metrics_list=metrics_list,\n",
    "        search_type=search_type,\n",
    "        n_iter=n_iter,\n",
    "        random_state=random_state,\n",
    "        n_jobs=n_jobs)\n",
    "\n",
    "\n",
    "    performance_test = prequential_grid_search(\n",
    "        data,\n",
    "        clf,\n",
    "        params,\n",
    "        features,\n",
    "        target,\n",
    "        scoring,\n",
    "        start_date_training=train_start_test,\n",
    "        preprop_list=preprop_list,\n",
    "        n_folds=n_folds,\n",
    "        subset='Test',\n",
    "        delta_train=delta_train,\n",
    "        delta_delay=delta_delay,\n",
    "        delta_assessment=delta_assessment,\n",
    "        performance_metrics_list_grid=metrics_list_grid,\n",
    "        performance_metrics_list=metrics_list,\n",
    "        search_type=search_type,\n",
    "        n_iter=n_iter,\n",
    "        random_state=random_state,\n",
    "        n_jobs=n_jobs)\n",
    "\n",
    "    cols_drop = ['Parameters', 'Fit time', 'Score time']\n",
    "    performance_validation.drop(columns=cols_drop, inplace=True)\n",
    "\n",
    "    performance = pd.concat([performance_test, performance_validation], axis=1)\n",
    "\n",
    "    summary_series = performance['Parameters']\n",
    "    params_keys_list = list(params.keys())\n",
    "    summary_params = [params_keys_list[i] for i in key_params_idxs]\n",
    "    \n",
    "    summary_list = []\n",
    "\n",
    "    for params_value in summary_series:\n",
    "        \n",
    "        summary_value_list = [params_value[i] for i in summary_params]\n",
    "        summary_value = '/'.join(map(str, summary_value_list))\n",
    "        summary_list.append(summary_value)\n",
    "        \n",
    "    performance['Parameters summary'] = summary_list\n",
    "    \n",
    "    if time_exec:\n",
    "        e_time = time.time() - t_start\n",
    "        e_time_format = str(dt.timedelta(seconds=np.round(e_time)))\n",
    "        print('Execution time: {}'.format(e_time_format))\n",
    "\n",
    "    return performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_summary_performances\n",
    "\n",
    "First use in [Chapter 5, Model Selection](Model_Selection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary_performances(performances_df, metrics):\n",
    "\n",
    "    # metrics = ['AUC ROC','Average precision','Card Precision@100']\n",
    "    metrics_test = [(i + ' Test') for i in metrics]\n",
    "    \n",
    "    performances_results=pd.DataFrame(columns=metrics)\n",
    "    \n",
    "    performances_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    best_estimated_parameters = []\n",
    "    validation_performance = []\n",
    "    test_performance = []\n",
    "    \n",
    "    for metric in metrics:\n",
    "\n",
    "        metric_max = np.argmax(performances_df[metric + ' Validation'].values)\n",
    "        \n",
    "        idx_best_valid = performances_df.index[metric_max]\n",
    "\n",
    "        metric_params = (performances_df['Parameters summary']\n",
    "                         .iloc[idx_best_valid])\n",
    "        \n",
    "        best_estimated_parameters.append(metric_params)\n",
    "\n",
    "        perf_valid = (performances_df[metric + ' Validation']\n",
    "                      .iloc[idx_best_valid])\n",
    "        \n",
    "        perf_valid_std = (performances_df[metric + ' Validation' + ' Std']\n",
    "                          .iloc[idx_best_valid])\n",
    "\n",
    "        perf_valid = '%.3f' % round(perf_valid, 3)\n",
    "        perf_valid_std = '%.3f' % round(perf_valid_std, 3)\n",
    "        \n",
    "        validation_performance.append(perf_valid + '+/-' + perf_valid_std)\n",
    "\n",
    "        perf_test = (performances_df[metric + ' Test']\n",
    "                     .iloc[idx_best_valid])\n",
    "\n",
    "        perf_test_std = (performances_df[metric + ' Test' + ' Std']\n",
    "                         .iloc[idx_best_valid])\n",
    "\n",
    "        perf_test = '%.3f' % round(perf_test, 3)\n",
    "        perf_test_std = '%.3f' % round(perf_test_std, 3)\n",
    "        \n",
    "        test_performance.append(perf_test + '+/-' + perf_test_std)\n",
    "    \n",
    "    performances_results.loc[\"Best estimated parameters\"] = best_estimated_parameters\n",
    "    performances_results.loc[\"Validation performance\"] = validation_performance\n",
    "    performances_results.loc[\"Test performance\"] = test_performance\n",
    "\n",
    "    optimal_test_performance = []\n",
    "    optimal_parameters = []\n",
    "\n",
    "    for metric in metrics_test:\n",
    "\n",
    "        metric_max_test = np.argmax(performances_df[metric].values)\n",
    "        idx_opt_test = performances_df.index[metric_max_test]\n",
    "\n",
    "        metric_params_test = (performances_df['Parameters summary']\n",
    "                              .iloc[idx_opt_test])\n",
    "    \n",
    "        optimal_parameters.append(metric_params_test)\n",
    "\n",
    "        perf_opt_test = performances_df[metric].iloc[idx_opt_test]\n",
    "        perf_opt_test_std = performances_df[metric + ' Std'].iloc[idx_opt_test]\n",
    "\n",
    "        perf_opt_test = '%.3f' % round(perf_opt_test, 3)\n",
    "        perf_opt_test_std = '%.3f' % round(perf_opt_test_std, 3)\n",
    "        \n",
    "        optimal_test_performance.append(perf_opt_test + '+/-' + perf_opt_test_std)\n",
    "\n",
    "    performances_results.loc[\"Optimal parameter(s)\"] = optimal_parameters\n",
    "    performances_results.loc[\"Optimal test performance\"] = optimal_test_performance\n",
    "    \n",
    "    return performances_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf_matrix(y_true, y_score, save=False, filename='0'):\n",
    "\n",
    "    matrix = confusion_matrix(y_true, y_score)\n",
    "\n",
    "    tp = matrix[1][1]\n",
    "    tn = matrix[0][0]\n",
    "    fp = matrix[0][1]\n",
    "    fn = matrix[1][0]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(4, 2))\n",
    "\n",
    "    ax = sns.heatmap(\n",
    "        matrix,\n",
    "        cmap=gradient,\n",
    "        vmin=0,\n",
    "        vmax=0,\n",
    "        annot_kws={'size': 12},\n",
    "        cbar_kws={'shrink': 1},\n",
    "        annot=True,\n",
    "        xticklabels=['NO', 'YES'],\n",
    "        yticklabels=['NO', 'YES'],\n",
    "        cbar=False,\n",
    "        linewidths=0.5,\n",
    "        linecolor='0.75',\n",
    "        fmt='g'\n",
    "    )\n",
    "\n",
    "    plt.xticks(size=9, rotation=0, y=-0.03)\n",
    "    plt.yticks(size=9, rotation=0, x=-0.01)\n",
    "\n",
    "    ax.set_ylabel('Actual', fontsize=9)\n",
    "    ax.set_xlabel('Predicted', fontsize=9)\n",
    "    ax.tick_params(left=True, bottom=True)\n",
    "    \n",
    "    ax.spines[:].set_visible(True)\n",
    "    ax.spines[:].set_linewidth(1)\n",
    "    ax.spines[:].set_color('0.35')\n",
    "\n",
    "    if save:\n",
    "        plt.savefig(\n",
    "            fname='img/{}.png'.format(filename),\n",
    "            bbox_inches='tight'\n",
    "        )\n",
    "        \n",
    "    plt.show()\n",
    "\n",
    "    return tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### threshold_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_range(y_true, y_score_prob, lower=0.2,\n",
    "                    upper=0.5, step=0.05, kind='prob'):\n",
    "\n",
    "    arr = np.arange(lower, upper, step)\n",
    "    df = pd.DataFrame(\n",
    "        columns={\n",
    "            'Threshold': float,\n",
    "            'Precision': None,\n",
    "            'Recall': None,\n",
    "            'F1-score': None,\n",
    "            'FP': int,\n",
    "            'FN': None,\n",
    "            'TP': None,\n",
    "            'TN': None\n",
    "            }\n",
    "        )\n",
    "\n",
    "    if kind == 'prob':\n",
    "        \n",
    "        for threshold in arr:\n",
    "            y_pred_lower_threshold = y_score_prob[:, 1]>=threshold\n",
    "            p = precision_score(y_true, y_pred_lower_threshold).round(2)\n",
    "            r = recall_score(y_true, y_pred_lower_threshold).round(2)\n",
    "            f1 = f1_score(y_true, y_pred_lower_threshold).round(2)\n",
    "\n",
    "            matrix = confusion_matrix(\n",
    "                y_true, \n",
    "                y_score_prob[:, 1] >= threshold\n",
    "            )\n",
    "\n",
    "            fp = matrix[0][1]\n",
    "            fn = matrix[1][0]\n",
    "            tp = matrix[1][1]\n",
    "            tn = matrix[0][0]\n",
    "\n",
    "            l = [threshold, p, r, f1, fp, fn, tp, tn]\n",
    "            df.loc[len(df)] = l\n",
    "\n",
    "        for col in ['FP', 'FN', 'TP', 'TN']:\n",
    "            df[col] = df[col].astype(np.int64)\n",
    "\n",
    "        return df\n",
    "\n",
    "    if kind == 'func':\n",
    "        \n",
    "        for threshold in arr:\n",
    "            y_pred_lower_threshold = y_score_prob>=threshold\n",
    "            p = precision_score(y_true, y_pred_lower_threshold).round(2)\n",
    "            r = recall_score(y_true, y_pred_lower_threshold).round(2)\n",
    "            f1 = f1_score(y_true, y_pred_lower_threshold).round(2)\n",
    "\n",
    "            j = round(threshold, 4)\n",
    "\n",
    "            l = [j, p, r, f1]\n",
    "            df.loc[len(df)] = l\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### results_df_create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_df_create(model_list, model_performance_list,\n",
    "                      model_exec_time_list, hyper_dict=None, drop_hyper=None):\n",
    "\n",
    "    final_df = pd.DataFrame(\n",
    "        columns={\n",
    "            'index': [],\n",
    "            'AUC ROC': [],\n",
    "            'AUC ROC Std': [],\n",
    "            'Average Precision': [],\n",
    "            'Average Precision Std': [],\n",
    "            'Card Precision@100': [],\n",
    "            'Card Precision@100 Std': [],\n",
    "            'Fit Time': [],\n",
    "            'Score Time': [],\n",
    "            'Tuning Time': [],\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    final_zip = zip(model_list,\n",
    "                    model_performance_list,\n",
    "                    model_exec_time_list)\n",
    "\n",
    "    for name, perf, time in final_zip:\n",
    "\n",
    "        final_df_row = []\n",
    "        df_sorted = perf.sort_values('Average Precision Validation',\n",
    "                                     ascending=False)\n",
    "        \n",
    "        auc_roc = df_sorted.iloc[0]['AUC ROC Test']\n",
    "        auc_roc_std = df_sorted.iloc[0]['AUC ROC Test Std']\n",
    "        avg_precision = df_sorted.iloc[0]['Average Precision Test']\n",
    "        avg_precision_std = df_sorted.iloc[0]['Average Precision Test Std']\n",
    "        card_precision = df_sorted.iloc[0]['Card Precision@100 Test']\n",
    "        card_precision_std = df_sorted.iloc[0]['Card Precision@100 Test Std']\n",
    "        fit_time = df_sorted.iloc[0]['Fit time']\n",
    "        score_time = df_sorted.iloc[0]['Score time']\n",
    "\n",
    "        # round metrics\n",
    "        round_3 = [\n",
    "            auc_roc,\n",
    "            avg_precision,\n",
    "            card_precision\n",
    "        ]\n",
    "        \n",
    "        (auc_roc,\n",
    "         avg_precision,\n",
    "         card_precision) = [np.round(i, 3) for i in round_3]\n",
    "\n",
    "        # round std deviations\n",
    "        round_4 = [\n",
    "            auc_roc_std,\n",
    "            avg_precision_std,\n",
    "            card_precision_std\n",
    "        ]\n",
    "\n",
    "        (auc_roc_std,\n",
    "         avg_precision_std,\n",
    "         card_precision_std) = [np.round(i, 4) for i in round_4]\n",
    "\n",
    "        # add rows to df\n",
    "        cols = [\n",
    "            name, auc_roc, auc_roc_std, avg_precision, avg_precision_std,\n",
    "            card_precision, card_precision_std, fit_time, score_time, time\n",
    "        ]\n",
    "        \n",
    "        for i in cols:\n",
    "            final_df_row.append(i)\n",
    "\n",
    "        final_df.loc[len(final_df)] = final_df_row\n",
    "\n",
    "        if hyper_dict:\n",
    "            \n",
    "            hyper_list = []\n",
    "            \n",
    "            for model in hyper_dict.keys():\n",
    "                params = list(hyper_dict[model].values())\n",
    "                if drop_hyper:\n",
    "                    del params[-drop_hyper:]\n",
    "                else:\n",
    "                    pass\n",
    "                    \n",
    "                hyper_list.append(params)\n",
    "                \n",
    "                hyper_string = []\n",
    "\n",
    "                for params in hyper_list:\n",
    "                    params_string = '/'.join(map(str, params))\n",
    "                    hyper_string.append(params_string)\n",
    "        \n",
    "    final_df = final_df.set_index('index', drop=True)  \n",
    "    final_df.index.name = None\n",
    "\n",
    "    if hyper_dict:\n",
    "\n",
    "        hyper_list = []\n",
    "        \n",
    "        cols = ['Parameters'] + list(final_df.columns)\n",
    "        final_df['Parameters'] = hyper_string\n",
    "        final_df = final_df[cols]\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract_hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hyperparams(dict, model, kind='value'):\n",
    "\n",
    "    dict_model = dict[model].copy()\n",
    "\n",
    "    if kind == 'list':\n",
    "        \n",
    "        dict_model_keys = dict_model.keys()\n",
    "        \n",
    "        for key in dict_model_keys:\n",
    "            dict_model[key] = [dict_model[key]]\n",
    "    \n",
    "    return dict_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data_smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_smoothing(data, x_var, y_vars, k=3, smooth_length=300):\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    for col in y_vars:\n",
    "    \n",
    "        x = data[x_var].values\n",
    "        y = data[col].values\n",
    "    \n",
    "        spl = make_interp_spline(x, y, k=k)\n",
    "        x_smooth = np.linspace(x.min(), x.max(), smooth_length) \n",
    "        y_smooth = spl(x_smooth)\n",
    "    \n",
    "        df[col] = y_smooth\n",
    "    \n",
    "    df[x_var] = x_smooth\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_smoothing(data, x_var, y_vars, k=3, smooth_num=300):\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    x = data[x_var].values\n",
    "    x_smooth = np.linspace(x.min(), x.max(), smooth_num)\n",
    "    \n",
    "    df[x_var] = x_smooth\n",
    "    \n",
    "    for col in y_vars:\n",
    "    \n",
    "        y = data[col].values\n",
    "        spl = make_interp_spline(x, y, k=k)\n",
    "        y_smooth = spl(x_smooth)\n",
    "    \n",
    "        df[col] = y_smooth\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_performance(data, metrics, subsets, \n",
    "                     title=None, xlabel=None, colors=None,\n",
    "                     rotation=None, save=False, filename='0'):\n",
    "\n",
    "    if not colors:\n",
    "        colors = palette\n",
    "\n",
    "    if not xlabel:\n",
    "        xlabel = 'Hyperparameter Value'\n",
    "    \n",
    "    cols = len(metrics)\n",
    "        \n",
    "    fig, axs = plt.subplots(nrows=1, ncols=cols,\n",
    "                            sharex=True, figsize=(11.5,2.5))\n",
    "    \n",
    "    fig.suptitle(title, weight='normal', size=11, y=1.1)\n",
    "    \n",
    "    abscissa = data['Parameters summary']\n",
    "    \n",
    "    for metric in metrics:\n",
    "    \n",
    "        ax_idx = metrics.index(metric)\n",
    "        \n",
    "        for subset in subsets:\n",
    "            \n",
    "            metric_subset = metric + ' ' + subset\n",
    "            metric_subset_std = metric_subset + ' Std'\n",
    "            \n",
    "            subset_idx = subsets.index(subset)\n",
    "    \n",
    "            # conf_min = data[metric_subset] - 2*data[metric_subset_std]\n",
    "            # conf_max = data[metric_subset] + 2*data[metric_subset_std]\n",
    "\n",
    "            # t-student = 3.18\n",
    "\n",
    "            conf_min = (data[metric_subset]\n",
    "                        - 3.18*(data[metric_subset_std]/4**0.5))\n",
    "            conf_max = (data[metric_subset]\n",
    "                        + 3.18*(data[metric_subset_std]/4**0.5))\n",
    "    \n",
    "            legend, label = (None, ) * 2\n",
    "            \n",
    "            if ax_idx == cols-1:\n",
    "                legend = 'brief'\n",
    "                label = subset\n",
    "            try:\n",
    "                sns.lineplot(\n",
    "                    data=data,\n",
    "                    x=abscissa,\n",
    "                    y=metric_subset,\n",
    "                    legend=legend,\n",
    "                    label=label,\n",
    "                    color=colors[subset_idx],\n",
    "                    ax=axs[ax_idx]\n",
    "                );\n",
    "\n",
    "            except TypeError:\n",
    "                print('Specify colors')\n",
    "                plt.close()\n",
    "                return\n",
    "            \n",
    "            axs[ax_idx].fill_between(abscissa,\n",
    "                                     conf_min,\n",
    "                                     conf_max,\n",
    "                                     color=colors[subset_idx],\n",
    "                                     alpha=0.07)\n",
    "            \n",
    "        # optimum - parameters for max value of each metric due Validation\n",
    "        metric_name = metric + ' ' + 'Validation'\n",
    "        best_index = data[metric_name].idxmax()\n",
    "        \n",
    "        best_parameter = data.loc[best_index, 'Parameters summary']\n",
    "        best_performance = data.loc[best_index, metric_name]\n",
    "    \n",
    "        ymin = axs[ax_idx].get_ylim()[0]\n",
    "        \n",
    "        # optimum line\n",
    "        axs[ax_idx].vlines(\n",
    "            best_parameter,\n",
    "            ymin,\n",
    "            best_performance,\n",
    "            linestyles='--',\n",
    "            color=palette[-1]\n",
    "        )\n",
    "\n",
    "        axs[ax_idx].set_title(metric,\n",
    "                                weight='bold',\n",
    "                                size=9, y=1.03,\n",
    "                                loc='center')\n",
    "        \n",
    "        axs[ax_idx].set_xlabel(xlabel=xlabel,\n",
    "                                 weight='normal',\n",
    "                                 style='italic',\n",
    "                                 labelpad=10)\n",
    "        \n",
    "        axs[ax_idx].set_ylabel(None)\n",
    "\n",
    "        if rotation:\n",
    "            axs[ax_idx].tick_params(axis='x', rotation=rotation)\n",
    "            anchor = (-0.33, -0.5)\n",
    "        else:\n",
    "            anchor = (-0.33, -0.32)\n",
    "\n",
    "    plt.legend(\n",
    "        labels=None, bbox_to_anchor=anchor,\n",
    "        ncols=2, prop={'size': 9})\n",
    "        \n",
    "    plt.subplots_adjust(wspace=0.2)\n",
    "\n",
    "    if save:\n",
    "        plt.savefig(\n",
    "            fname='img/{}.png'.format(filename),\n",
    "            bbox_inches='tight'\n",
    "        )\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(data, metrics, models_list, palette, limits_list, save=False, filename='0'):\n",
    "\n",
    "    width=0.55\n",
    "    fig_width = 11\n",
    "    fig_height = 3\n",
    "    t = 3.18\n",
    "    n_folds = 4\n",
    "\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(fig_width,fig_height))\n",
    "\n",
    "    for metric in metrics:\n",
    "\n",
    "        metric_index = metrics.index(metric)\n",
    "        \n",
    "        sns.barplot(\n",
    "            data=data,\n",
    "            x=results.index,\n",
    "            y=metric,\n",
    "            width=width,\n",
    "            palette=palette,\n",
    "            ax=axs[metric_index]\n",
    "        );\n",
    "\n",
    "        ymin = limits_list[metric_index][0]\n",
    "        ymax = limits_list[metric_index][1]\n",
    "        step = limits_list[metric_index][2]\n",
    "\n",
    "        axs[metric_index].tick_params(\n",
    "            axis='x',\n",
    "            which='both',\n",
    "            bottom=False,\n",
    "            top=False,\n",
    "            labelbottom=False)\n",
    "        \n",
    "        axs[metric_index].set_ylim(ymin, ymax)\n",
    "        axs[metric_index].set_yticks(ticks=np.arange(ymin, ymax, step))\n",
    "        axs[metric_index].yaxis.set_tick_params(labelsize=8)\n",
    "        # axs[metric_index].xaxis.set_tick_params(rotation=rotation)\n",
    "        axs[metric_index].set_title(metric, size=9)\n",
    "        axs[metric_index].set_ylabel(None)\n",
    "\n",
    "        for model in data.index:\n",
    "\n",
    "            mean = data.loc[model, metric]\n",
    "            \n",
    "            metric_std_col = metric + ' Std'\n",
    "            std = data.loc[model, metric_std_col]\n",
    "            \n",
    "            ymin = mean - t*(std/n_folds**0.5)\n",
    "            ymax = mean + t*(std/n_folds**0.5)\n",
    "            \n",
    "            axs[metric_index].plot([model, model], [ymin, ymax],\n",
    "                     color=palette[-1],\n",
    "                     linestyle='-',\n",
    "                     linewidth=1.5)\n",
    "    \n",
    "    lr = mlines.Line2D([], [], color=colors[0], marker='s', linestyle='None',\n",
    "                          markersize=5, label=models_list[0])\n",
    "\n",
    "    rf = mlines.Line2D([], [], color=colors[1], marker='s', linestyle='None',\n",
    "                          markersize=5, label=models_list[1])\n",
    "\n",
    "    xgb = mlines.Line2D([], [], color=colors[2], marker='s', linestyle='None',\n",
    "                          markersize=5, label=models_list[2])\n",
    "\n",
    "    lgb = mlines.Line2D([], [], color=colors[3], marker='s', linestyle='None',\n",
    "                          markersize=5, label=models_list[3])\n",
    "    \n",
    "    plt.legend(\n",
    "        labels=None, loc='lower center', bbox_to_anchor=(-0.8, -0.35), ncols=2,\n",
    "        prop={'size': 9}, labelcolor='0.3', handles=[lr,rf,xgb,lgb]\n",
    "    )\n",
    "    \n",
    "    plt.subplots_adjust(wspace=0.3)\n",
    "    \n",
    "    if save:\n",
    "        plt.savefig(\n",
    "            fname='img/{}.png'.format(filename),\n",
    "            bbox_inches='tight'\n",
    "        )\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot_roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(\n",
    "        y_true, y_score_prob_list, name_list,\n",
    "        palette, save=True, filename='0'):\n",
    "\n",
    "    markers = ['o', 'v', 's', 'D']\n",
    "    vars_zip = zip(y_score_prob_list, name_list, palette, markers)\n",
    "    \n",
    "    plt.figure(figsize=(8, 4))\n",
    "    \n",
    "    for score, name, color, marker in vars_zip:\n",
    "\n",
    "        fpr, tpr, thresholds = roc_curve(y_true, score)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        label = '{0} (AUC = {1:.3f})'.format(name, roc_auc)\n",
    "\n",
    "        # roc_curve\n",
    "        plt.plot(fpr, tpr, label=label, color=color)\n",
    "\n",
    "        # random model curve\n",
    "        plt.plot([0, 1], [0, 1],\n",
    "                 color=palette[-1],\n",
    "                 linestyle='--',\n",
    "                 linewidth=0.6)\n",
    "        \n",
    "        plt.xlabel(\n",
    "            xlabel='False Positive Rate (1-Specifity)', fontsize=10,\n",
    "            weight='normal', color='0.3'\n",
    "        )\n",
    "        \n",
    "        plt.ylabel(\n",
    "            ylabel='True Positive Rate (Recall)', fontsize=10,\n",
    "            weight='normal', color='0.3'\n",
    "        )\n",
    "\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.08])\n",
    "        plt.xticks(size=9)\n",
    "        plt.yticks(size=9)\n",
    "    \n",
    "        close_default = np.argmin(np.abs(thresholds - 0.5))\n",
    "        \n",
    "        sns.scatterplot(\n",
    "            x=[fpr[close_default]],\n",
    "            y=[tpr[close_default]],\n",
    "            marker=marker,\n",
    "            s=50,\n",
    "            color=palette[-1],\n",
    "            edgecolor=palette[-1],\n",
    "            linewidth=1.5,\n",
    "            facecolor='None',\n",
    "            label='0.5 Predict Probability Threshold'\n",
    "        )\n",
    "    \n",
    "    plt.legend(\n",
    "        labels=None, loc='upper right', bbox_to_anchor=(1.435, 1),\n",
    "        prop={'size': 9}, labelcolor='0.3'\n",
    "    )\n",
    "\n",
    "    if save:\n",
    "        plt.savefig(\n",
    "            fname='img/{}.png'.format(filename),\n",
    "            bbox_inches='tight'\n",
    "        )\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot_prec_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prec_rec(\n",
    "        y_true, y_score_prob_list, name_list,\n",
    "        palette, save=False, filename='0'):\n",
    "\n",
    "    markers = ['o', 'v', 's', 'D']\n",
    "    vars_zip = zip(y_score_prob_list, name_list, palette, markers)\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    metrics_opt = []\n",
    "    \n",
    "    for score, name, color, marker in vars_zip:\n",
    "    \n",
    "        precision, recall, thresholds = precision_recall_curve(y_true, score)\n",
    "        # calculate f_score\n",
    "        f = (2 * precision * recall) / (precision + recall)\n",
    "        # locate the index of the largest f_score\n",
    "        x = np.argmax(f)\n",
    "        # calculate best recall, precision\n",
    "        precision_opt = round(precision[x], 2)\n",
    "        recall_opt = round(recall[x], 2)\n",
    "        threshold_opt = round(thresholds[x], 2)\n",
    "\n",
    "        ap = average_precision_score(y_true, score)\n",
    "\n",
    "        metrics_list = [precision_opt, recall_opt, threshold_opt]\n",
    "        metrics_opt.append(metrics_list)\n",
    "    \n",
    "        plt.xlabel('Recall', fontsize=10, weight='normal', color='0.3')\n",
    "        plt.ylabel('Precision', fontsize=10, weight='normal', color='0.3')\n",
    "    \n",
    "        plt.plot(recall, precision, label='{0} (AP = {1:.3f})'.format(name, ap), color=color)\n",
    "    \n",
    "        sns.scatterplot(\n",
    "            x=[recall_opt],\n",
    "            y=[precision_opt],\n",
    "            marker=marker,\n",
    "            s=50,\n",
    "            color=palette[-1],\n",
    "            edgecolor=palette[-1],\n",
    "            linewidth=1.5,\n",
    "            facecolor='None',\n",
    "            label='Optimal Threshold'\n",
    "        )\n",
    "\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.08])\n",
    "    plt.xticks(size=9)\n",
    "    plt.yticks(size=9)\n",
    "    \n",
    "    plt.legend(\n",
    "        labels=None, loc='upper right', bbox_to_anchor=(1.425, 1),\n",
    "        prop={'size': 9}, labelcolor='0.3'\n",
    "    )\n",
    "    \n",
    "    if save:\n",
    "        plt.savefig(\n",
    "            fname='img/{}.png'.format(filename),\n",
    "            bbox_inches='tight'\n",
    "        )\n",
    "        \n",
    "    plt.show()\n",
    "\n",
    "    return metrics_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_times(data, times, palette, figsize=(8,6), save=False, filename='0'):\n",
    "\n",
    "    width = 0.6\n",
    "    size = 9\n",
    "    nrows = len(times)\n",
    "    ncols = 1\n",
    "\n",
    "    fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize)\n",
    "\n",
    "    for time in times:\n",
    "\n",
    "        time_index = times.index(time)\n",
    "\n",
    "        if len(times) == 1:\n",
    "            ax=axs\n",
    "        else:\n",
    "            ax=axs[time_index]\n",
    "\n",
    "        sns.barplot(\n",
    "            data=data,\n",
    "            x=time,\n",
    "            y=data.index,\n",
    "            palette=palette,\n",
    "            width=width,\n",
    "            ax=ax\n",
    "        )\n",
    "        \n",
    "        ax.set_xlabel(None)\n",
    "        ax.set_title(\n",
    "            label='{0} (sec)'.format(time), weight='bold', size=size\n",
    "        )\n",
    "        \n",
    "    plt.subplots_adjust(hspace=0.6)\n",
    "\n",
    "    if save:\n",
    "        plt.savefig(\n",
    "            fname='img/{}.png'.format(filename),\n",
    "            bbox_inches='tight'\n",
    "        )\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot_unbalanced_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_unbalanced_model(\n",
    "        data, metrics_list, colors,\n",
    "        n_folds, limits_list, save=False, filename='0'):\n",
    "\n",
    "    linewidth = 1.5\n",
    "    alpha = 0.25\n",
    "    t = 3.18\n",
    "    \n",
    "    plt.figure(figsize=(11.5,5))\n",
    "\n",
    "    rows = int(np.ceil(len(metrics_list) / 2))\n",
    "    cols = 2\n",
    "\n",
    "    for metric in metrics_list:\n",
    "\n",
    "        metric_index = metrics_list.index(metric)\n",
    "        \n",
    "        plt.subplot(rows,cols,metric_index+1)\n",
    "        plt.title(metric, size=9)\n",
    "        \n",
    "        sns.lineplot(\n",
    "            data=data,\n",
    "            x=data.index,\n",
    "            y=metric,\n",
    "            linewidth=linewidth,\n",
    "            alpha=alpha,\n",
    "            color=colors[metric_index]\n",
    "        );\n",
    "\n",
    "        sns.scatterplot(\n",
    "            data=data,\n",
    "            x=data.index,\n",
    "            y=metric,\n",
    "            s=25,\n",
    "            alpha=1,\n",
    "            color=colors[metric_index]\n",
    "        )\n",
    "\n",
    "        column_std_list = list(data.columns)\n",
    "        metric_std = metric + ' Std'\n",
    "        \n",
    "        if metric_std in column_std_list:\n",
    "        \n",
    "            for model in data.index:\n",
    "    \n",
    "                metric_std = metric + ' Std'\n",
    "                \n",
    "                mean = data.loc[model, metric]\n",
    "                std = data.loc[model, metric_std]\n",
    "                \n",
    "                ymin = mean - t*(std/n_folds**0.5)\n",
    "                ymax = mean + t*(std/n_folds**0.5)\n",
    "        \n",
    "                # plot confidence intervals\n",
    "                plt.plot([model, model],\n",
    "                         [ymin, ymax],\n",
    "                          color=colors[metric_index],\n",
    "                          linewidth=1.4,\n",
    "                          alpha=0.75)\n",
    "        \n",
    "        ymin = np.arange(limits_list[metric_index][0],\n",
    "                         limits_list[metric_index][1],\n",
    "                         limits_list[metric_index][2])[0]\n",
    "        ymax = np.arange(limits_list[metric_index][0],\n",
    "                         limits_list[metric_index][1],\n",
    "                         limits_list[metric_index][2])[-1]\n",
    "            \n",
    "        plt.ylim(ymin, ymax)\n",
    "        plt.yticks(\n",
    "            np.arange(limits_list[metric_index][0],\n",
    "                      limits_list[metric_index][1],\n",
    "                      limits_list[metric_index][2])\n",
    "        )\n",
    "        plt.ylabel(None)\n",
    "        \n",
    "    plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
    "\n",
    "    if save:\n",
    "        plt.savefig(\n",
    "            fname='img/{}.png'.format(filename),\n",
    "            bbox_inches='tight'\n",
    "        )\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot_unbalanced_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_unbalanced_metric(data, metric, colors, models_list, save=False, filename='0'):\n",
    "\n",
    "    plt.figure(figsize=(8,4))\n",
    "    \n",
    "    sns.lineplot(\n",
    "        data=data,\n",
    "        x=data.index,\n",
    "        y=metric,\n",
    "        hue='model',\n",
    "        legend=False,\n",
    "        palette=colors,\n",
    "        alpha=0.25,\n",
    "        linewidth=1.5\n",
    "    )\n",
    "    \n",
    "    sns.scatterplot(\n",
    "        data=data,\n",
    "        x=data.index,\n",
    "        y=metric,\n",
    "        hue='model',\n",
    "        palette=colors,\n",
    "        marker='s',\n",
    "        s=40,\n",
    "        alpha=1\n",
    "    )\n",
    "\n",
    "    lr = mlines.Line2D([], [], color=colors[0], marker='s', linestyle='None',\n",
    "                          markersize=5, label=models_list[0])\n",
    "\n",
    "    rf = mlines.Line2D([], [], color=colors[1], marker='s', linestyle='None',\n",
    "                          markersize=5, label=models_list[1])\n",
    "\n",
    "    xgb = mlines.Line2D([], [], color=colors[2], marker='s', linestyle='None',\n",
    "                          markersize=5, label=models_list[2])\n",
    "\n",
    "    lgb = mlines.Line2D([], [], color=colors[3], marker='s', linestyle='None',\n",
    "                          markersize=5, label=models_list[3])\n",
    "    \n",
    "    plt.legend(\n",
    "        labels=None, loc='upper right', bbox_to_anchor=(1.42, 1.025),\n",
    "        prop={'size': 9}, labelcolor='0.3', handles=[lr,rf,xgb,lgb]\n",
    "    )\n",
    "\n",
    "    plt.ylabel(metric, weight='normal', size=10)\n",
    "\n",
    "    if save:\n",
    "        plt.savefig(\n",
    "            fname='img/{}.png'.format(filename),\n",
    "            bbox_inches='tight'\n",
    "        )\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
