{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate_customer_profiles_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_customer_profiles_table(n_customers, random_state=42):\n",
    "    \n",
    "    np.random.seed(random_state)\n",
    "    customer_id_properties = []\n",
    "    \n",
    "    for customer_id in range(n_customers):\n",
    "        \n",
    "        x_customer_id = np.random.uniform(0,100)\n",
    "        y_customer_id = np.random.uniform(0,100)\n",
    "        \n",
    "        mean_amount = np.random.uniform(5,100)\n",
    "        std_amount = mean_amount/2\n",
    "        \n",
    "        mean_nb_tx_per_day = np.random.uniform(0,4)\n",
    "        \n",
    "        customer_id_properties.append([\n",
    "            customer_id,\n",
    "            x_customer_id,\n",
    "            y_customer_id,\n",
    "            mean_amount,\n",
    "            std_amount,\n",
    "            mean_nb_tx_per_day\n",
    "        ])\n",
    "        \n",
    "    customer_profiles_table = pd.DataFrame(\n",
    "        data=customer_id_properties,\n",
    "        columns=[\n",
    "            'customer_id',\n",
    "            'x_customer_id',\n",
    "            'y_customer_id',\n",
    "            'mean_amount',\n",
    "            'std_amount',\n",
    "            'mean_nb_tx_per_day'\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return customer_profiles_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate_terminal_profiles_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_terminal_profiles_table(n_terminals, random_state=42):\n",
    "    \n",
    "    np.random.seed(random_state)\n",
    "    terminal_id_properties = []\n",
    "    \n",
    "    for terminal_id in range(n_terminals):\n",
    "        \n",
    "        x_terminal_id = np.random.uniform(0,100)\n",
    "        y_terminal_id = np.random.uniform(0,100)\n",
    "        \n",
    "        terminal_id_properties.append([\n",
    "            terminal_id,\n",
    "            x_terminal_id,\n",
    "            y_terminal_id\n",
    "        ])               \n",
    "        \n",
    "    terminal_profiles_table = pd.DataFrame(\n",
    "        data=terminal_id_properties,\n",
    "        columns=[\n",
    "            'terminal_id',\n",
    "            'x_terminal_id',\n",
    "            'y_terminal_id'\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return terminal_profiles_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_list_terminals_within_radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_terminals_within_radius(customer_profile, x_y_terminals, r):\n",
    "    \n",
    "    x_y_customer = (customer_profile[['x_customer_id','y_customer_id']]\n",
    "                    .values\n",
    "                    .astype(float))\n",
    "    \n",
    "    squared_diff_x_y = np.square(x_y_customer - x_y_terminals)\n",
    "    dist_x_y = np.sqrt(np.sum(squared_diff_x_y, axis=1))\n",
    "    available_terminals = list(np.where(dist_x_y < r)[0])\n",
    "    \n",
    "    return available_terminals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate_transactions_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_transactions_table(customer_profile, nb_days=10):\n",
    "    \n",
    "    customer_transactions = []\n",
    "    \n",
    "    random.seed(int(customer_profile['customer_id']))\n",
    "    np.random.seed(int(customer_profile['customer_id']))\n",
    "    \n",
    "    # For all days\n",
    "    for day in range(nb_days):\n",
    "        # Random number of transactions for that day \n",
    "        nb_tx = np.random.poisson(customer_profile['mean_nb_tx_per_day'])\n",
    "        # If nb_tx positive, let us generate transactions\n",
    "        if nb_tx > 0:\n",
    "            for tx in range(nb_tx):\n",
    "                # Time of transaction: Around noon, std 20000 seconds. \n",
    "                # This choice aims at simulating the fact that \n",
    "                # most transactions occur during the day.\n",
    "                time_tx = int(np.random.normal(86400 / 2, 20000))\n",
    "                # If transaction time between 0 and 86400, \n",
    "                # let us keep it, otherwise, let us discard it\n",
    "\n",
    "                if (time_tx > 0 and \n",
    "                    time_tx < 86400):\n",
    "                    # Amount is drawn from a normal distribution  \n",
    "                    amount = np.random.normal(\n",
    "                        customer_profile['mean_amount'], \n",
    "                        customer_profile['std_amount']\n",
    "                    )\n",
    "                    # If amount negative, draw from a uniform distribution\n",
    "                    if amount < 0:\n",
    "                        amount = np.random.uniform(\n",
    "                            0, customer_profile['mean_amount']*2)\n",
    "                        \n",
    "                    amount = np.round(amount, decimals=2)\n",
    "                    \n",
    "                    if len(customer_profile['available_terminals']) > 0:\n",
    "                        terminal_id = random.choice(\n",
    "                            customer_profile['available_terminals'])\n",
    "                        \n",
    "                        customer_transactions.append([\n",
    "                            time_tx+day*86400,\n",
    "                            day,\n",
    "                            customer_profile['customer_id'],\n",
    "                            terminal_id,\n",
    "                            amount\n",
    "                        ])\n",
    "            \n",
    "    customer_transactions = pd.DataFrame(\n",
    "        data=customer_transactions,\n",
    "        columns=[\n",
    "            'tx_time_seconds',\n",
    "            'tx_time_days',\n",
    "            'customer_id',\n",
    "            'terminal_id',\n",
    "            'tx_amount'\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    if len(customer_transactions) > 0:\n",
    "        \n",
    "        customer_transactions['tx_datetime'] = pd.to_datetime(\n",
    "            arg=customer_transactions[\"tx_time_seconds\"],\n",
    "            unit='s',\n",
    "            origin=start_date\n",
    "        )\n",
    "\n",
    "        customer_transactions = customer_transactions[[\n",
    "            'tx_datetime',\n",
    "            'customer_id',\n",
    "            'terminal_id',\n",
    "            'tx_amount',\n",
    "            'tx_time_seconds',\n",
    "            'tx_time_days'\n",
    "        ]]\n",
    "    \n",
    "    return customer_transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add_frauds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_frauds(customer_profiles_table, terminal_profiles_table, data):\n",
    "    \n",
    "    # By default, all transactions are genuine\n",
    "    data['tx_fraud'] = 0\n",
    "    data['tx_fraud_scenario'] = 0\n",
    "    \n",
    "    # Scenario 1\n",
    "    data.loc[data['tx_amount']>220, 'tx_fraud'] = 1\n",
    "    data.loc[data['tx_amount']>220, 'tx_fraud_scenario'] = 1\n",
    "    nb_frauds_scenario_1 = data['tx_fraud'].sum()\n",
    "    \n",
    "    # Scenario 2\n",
    "    for day in range(data['tx_time_days'].max()):\n",
    "        \n",
    "        compromised_terminals = (terminal_profiles_table['terminal_id']\n",
    "                                 .sample(n=2, random_state=day))\n",
    "        \n",
    "        compromised_transactions = data[\n",
    "            (data['tx_time_days'] >= day) &\n",
    "            (data['tx_time_days'] < day+28) &\n",
    "            (data['terminal_id'].isin(compromised_terminals))\n",
    "        ]\n",
    "                            \n",
    "        data.loc[compromised_transactions.index, 'tx_fraud'] = 1\n",
    "        data.loc[compromised_transactions.index, 'tx_fraud_scenario'] = 2\n",
    "    \n",
    "    nb_frauds_scenario_2 = data['tx_fraud'].sum() - nb_frauds_scenario_1\n",
    "    \n",
    "    # Scenario 3\n",
    "    for day in range(data['tx_time_days'].max()):\n",
    "        \n",
    "        compromised_customers = (customer_profiles_table['customer_id']\n",
    "                                 .sample(n=3, random_state=day)\n",
    "                                 .values)\n",
    "        \n",
    "        compromised_transactions = data[\n",
    "            (data['tx_time_days'] >= day) &\n",
    "            (data['tx_time_days'] < day + 14) &\n",
    "            (data['customer_id'].isin(compromised_customers))\n",
    "        ]\n",
    "        \n",
    "        nb_compromised_transactions = len(compromised_transactions)\n",
    "        \n",
    "        random.seed(day)\n",
    "        \n",
    "        index_fauds = random.sample(\n",
    "            list(compromised_transactions.index.values), \n",
    "            k=int(nb_compromised_transactions/3)\n",
    "        )\n",
    "        \n",
    "        data.loc[index_fauds, 'tx_amount'] = (data\n",
    "                                              .loc[index_fauds, 'tx_amount']*5)\n",
    "        \n",
    "        data.loc[index_fauds, 'tx_fraud'] = 1\n",
    "        data.loc[index_fauds, 'tx_fraud_scenario'] = 3\n",
    "        \n",
    "                             \n",
    "    nb_frauds_scenario_3 = (data['tx_fraud'].sum()\n",
    "                            - nb_frauds_scenario_2\n",
    "                            - nb_frauds_scenario_1)\n",
    "    \n",
    "    return data                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(\n",
    "        start_date, random_state=42, n_customers=10000,\n",
    "        n_terminals=1000000, nb_days=90, r=5):\n",
    "    \n",
    "    customer_profiles_table = generate_customer_profiles_table(\n",
    "        n_customers, random_state=random_state)\n",
    "    \n",
    "    terminal_profiles_table = generate_terminal_profiles_table(\n",
    "        n_terminals, random_state=random_state+1)\n",
    "    \n",
    "    x_y_terminals = (terminal_profiles_table\n",
    "                     [['x_terminal_id', 'y_terminal_id']]\n",
    "                     .values\n",
    "                     .astype(float))\n",
    "\n",
    "    av_terminals = customer_profiles_table.apply(\n",
    "        lambda x : get_list_terminals_within_radius(x, x_y_terminals, r), axis=1\n",
    "    )\n",
    "    customer_profiles_table['available_terminals'] = av_terminals\n",
    "\n",
    "    nb_terminals = customer_profiles_table['available_terminals'].apply(len)\n",
    "    customer_profiles_table['nb_terminals'] = nb_terminals\n",
    "    \n",
    "    df = (customer_profiles_table\n",
    "          .groupby('customer_id')\n",
    "          .apply(lambda x : generate_transactions_table(x.iloc[0], nb_days))\n",
    "          .reset_index(drop=True))\n",
    "    \n",
    "    # Sort transactions chronologically\n",
    "    df = df.sort_values('tx_datetime')\n",
    "    # Reset indices, starting from 0\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    df.reset_index(inplace=True)\n",
    "    df.rename(columns={'index':'transaction_id'}, inplace=True)\n",
    "    \n",
    "    return customer_profiles_table, terminal_profiles_table, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read_from_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_files(DIR_INPUT, BEGIN_DATE, END_DATE, sort='transaction_id'):\n",
    "    \n",
    "    files = [os.path.join(DIR_INPUT, f) for f in os.listdir(DIR_INPUT) \\\n",
    "             if f >= BEGIN_DATE + '.pkl' and f <= END_DATE + '.pkl']\n",
    "\n",
    "    frames = []\n",
    "    \n",
    "    for f in files:\n",
    "        df = pd.read_pickle(f)\n",
    "        frames.append(df)\n",
    "        del df\n",
    "        \n",
    "    df_final = pd.concat(frames)\n",
    "    df_final = df_final.sort_values(sort)\n",
    "    df_final.reset_index(drop=True, inplace=True)\n",
    "    #  Note: -1 are missing values for real world data \n",
    "    df_final = df_final.replace([-1],0)\n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save_object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save oject as pickle file\n",
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as output:\n",
    "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### is_weekend, is_night"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_weekend(tx_datetime):\n",
    "    \n",
    "    # Transform date into weekday (0 is Monday, 6 is Sunday)\n",
    "    weekday = tx_datetime.weekday()\n",
    "    # Binary value: 0 if weekday, 1 if weekend\n",
    "    is_weekend = weekday>=5\n",
    "    \n",
    "    return int(is_weekend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_night(tx_datetime):\n",
    "    \n",
    "    # Get the hour of the transaction\n",
    "    tx_hour = tx_datetime.hour\n",
    "    # Binary value: 1 if hour less than 6, and 0 otherwise\n",
    "    is_night = tx_hour<=6\n",
    "    \n",
    "    return int(is_night)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_customer_spending_behaviour_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_customer_spending_behaviour_features(\n",
    "        customer_transactions, windows_size_in_days=[1,7,30]):\n",
    "    \n",
    "    # Let us first order transactions chronologically\n",
    "    customer_transactions = customer_transactions.sort_values('tx_datetime')\n",
    "    # The transaction date and time is set as the index, \n",
    "    # which will allow the use of the rolling function \n",
    "    customer_transactions.index = customer_transactions['tx_datetime']\n",
    "    # For each window size\n",
    "    for window_size in windows_size_in_days:\n",
    "        # Compute the sum of the transaction amounts and \n",
    "        # the number of transactions for the given window size\n",
    "        sum_amount_tx_window = (customer_transactions['tx_amount']\n",
    "                                .rolling(str(window_size)+'d')\n",
    "                                .sum())\n",
    "        \n",
    "        nb_tx_window = (customer_transactions['tx_amount']\n",
    "                        .rolling(str(window_size)+'d')\n",
    "                        .count())\n",
    "    \n",
    "        # Compute the average transaction amount for the given window size\n",
    "        # 'nb_tx_window' is always >0 since current transaction is always included\n",
    "        avg_amount_tx_window = sum_amount_tx_window / nb_tx_window\n",
    "    \n",
    "        # Save feature values\n",
    "        customer_nb_name = ('customer_id_nb_tx_'\n",
    "                            + str(window_size)\n",
    "                            + 'day_window')\n",
    "        \n",
    "        customer_avg_name = ('customer_id_avg_amount_'\n",
    "                             + str(window_size)\n",
    "                             + 'day_window')\n",
    "        \n",
    "        customer_transactions[customer_nb_name] = list(nb_tx_window)\n",
    "        customer_transactions[customer_avg_name] = list(avg_amount_tx_window)\n",
    "\n",
    "    customer_transactions.index = customer_transactions['transaction_id']\n",
    "    \n",
    "    return customer_transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_count_risk_rolling_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count_risk_rolling_window(\n",
    "        terminal_transactions, delay_period=7,\n",
    "        windows_size_in_days=[1,7,30], feature='terminal_id'):\n",
    "    \n",
    "    terminal_transactions = terminal_transactions.sort_values('tx_datetime')\n",
    "    terminal_transactions.index = terminal_transactions['tx_datetime']\n",
    "\n",
    "    name_delay = str(delay_period) + 'd'\n",
    "    \n",
    "    nb_fraud_delay = (terminal_transactions['tx_fraud']\n",
    "                      .rolling(name_delay)\n",
    "                      .sum())\n",
    "\n",
    "    nb_tx_delay = (terminal_transactions['tx_fraud']\n",
    "                   .rolling(name_delay)\n",
    "                   .count())\n",
    "    \n",
    "    for window_size in windows_size_in_days:\n",
    "\n",
    "        name_window = str(delay_period+window_size) + 'd'\n",
    "        \n",
    "        nb_fraud_delay_window = (terminal_transactions['tx_fraud']\n",
    "                                 .rolling(name_window)\n",
    "                                 .sum())\n",
    "        \n",
    "        nb_tx_delay_window = (terminal_transactions['tx_fraud']\n",
    "                              .rolling(name_window)\n",
    "                              .count())\n",
    "    \n",
    "        nb_fraud_window = nb_fraud_delay_window - nb_fraud_delay\n",
    "        nb_tx_window = nb_tx_delay_window - nb_tx_delay\n",
    "        risk_window = nb_fraud_window / nb_tx_window\n",
    "\n",
    "        name_nb = feature + '_nb_tx_' + str(window_size) + 'day_window'\n",
    "        name_risk = feature + '_risk_' + str(window_size) + 'day_window'\n",
    "        \n",
    "        terminal_transactions[name_nb] = list(nb_tx_window)\n",
    "        terminal_transactions[name_risk] = list(risk_window)\n",
    "        \n",
    "    terminal_transactions.index = terminal_transactions['transaction_id']\n",
    "    \n",
    "    terminal_transactions.fillna(0, inplace=True)\n",
    "    \n",
    "    return terminal_transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_train_test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_set(\n",
    "        data, start_date_training, delta_train=7, delta_delay=7,\n",
    "        delta_test=7, sampling_ratio=1.0, random_state=42):\n",
    "\n",
    "    end_date_training = (start_date_training\n",
    "                        + dt.timedelta(days=delta_train))\n",
    "    \n",
    "    # Get the training set data\n",
    "    train_df = data[\n",
    "        (data['tx_datetime'] >= start_date_training)\n",
    "        & (data['tx_datetime'] < end_date_training)\n",
    "    ].copy()\n",
    "    \n",
    "    # Get the test set data\n",
    "    test_df = []\n",
    "    \n",
    "    # Note: Cards known to be compromised after \n",
    "    # the delay period are removed from the test set\n",
    "    # That is, for each test day, all frauds known at (test_day-delay_period) \n",
    "    # are removed\n",
    "    \n",
    "    # First, get known defrauded customers from the training set\n",
    "    known_defrauded_customers = train_df.loc[\n",
    "        train_df['tx_fraud']==1, 'customer_id']\n",
    "    # drop duplicates\n",
    "    known_defrauded_customers = set(known_defrauded_customers)\n",
    "    \n",
    "    # Get the relative starting day of training set \n",
    "    # (easier than TX_DATETIME to collect test data)\n",
    "    start_tx_time_days_training = train_df['tx_time_days'].min()\n",
    "    \n",
    "    # Then, for each day of the test set\n",
    "    for day in range(delta_test):\n",
    "        # current test day number since the start of training\n",
    "        test_day = sum([\n",
    "            start_tx_time_days_training,\n",
    "            delta_train,\n",
    "            delta_delay,\n",
    "            day\n",
    "        ])\n",
    "        \n",
    "        # get whole test data for that day\n",
    "        test_day_df = data[data['tx_time_days']==test_day]\n",
    "        \n",
    "        # find current day minus delay period\n",
    "        test_day_delay = test_day - delta_delay\n",
    "        \n",
    "        # get data for the day before it\n",
    "        slice = data['tx_time_days']==test_day_delay-1\n",
    "        test_day_df_delay = data[slice].copy()\n",
    "        # defrauded cards from that day, are added \n",
    "        # to the pool of known defrauded customers\n",
    "        loc = test_day_df_delay['tx_fraud']==1, 'customer_id'\n",
    "        new_defrauded_customers = test_day_df_delay.loc[loc]\n",
    "        # drop duplicates\n",
    "        new_defrauded_customers = set(new_defrauded_customers)\n",
    "        # add new defrauded customers from delay period to known defrauded customers\n",
    "        known_defrauded_customers = (known_defrauded_customers\n",
    "                                     .union(new_defrauded_customers))\n",
    "        # remove them from current test day data\n",
    "        known_defrauded_slice = (test_day_df['customer_id']\n",
    "                              .isin(known_defrauded_customers))\n",
    "        test_day_df = test_day_df[~known_defrauded_slice]\n",
    "        # add test data for this day for test subset\n",
    "        test_df.append(test_day_df)\n",
    "        \n",
    "    test_df = pd.concat(test_df)\n",
    "    \n",
    "    # If subsample\n",
    "    if sampling_ratio < 1:\n",
    "\n",
    "        tx_fraud_1 = train_df['tx_fraud'] == 1\n",
    "        tx_fraud_0 = train_df['tx_fraud'] == 0\n",
    "\n",
    "        # get sample of frauds\n",
    "        train_df_frauds = train_df[tx_fraud_1].sample(\n",
    "            frac=sampling_ratio, random_state=random_state)\n",
    "        # get sample of genuine\n",
    "        train_df_genuine = train_df[tx_fraud_0].sample(\n",
    "            frac=sampling_ratio, random_state=random_state)\n",
    "        \n",
    "        train_df = pd.concat([train_df_frauds, train_df_genuine])\n",
    "        \n",
    "    # Sort data sets by ascending order of transaction ID\n",
    "    train_df = train_df.sort_values('transaction_id')\n",
    "    test_df = test_df.sort_values('transaction_id')\n",
    "    \n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prequentialSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prequentialSplit(\n",
    "        data, start_date_training, n_folds=4,\n",
    "        delta_train=7, delta_delay=7, delta_assessment=7):\n",
    "    \n",
    "    prequential_split_indices=[]\n",
    "        \n",
    "    # For each fold\n",
    "    for fold in range(n_folds):\n",
    "        # Shift back start date for training by the fold index times\n",
    "        # the assessment period (delta_assessment)\n",
    "        delta = dt.timedelta(days=fold*delta_assessment)\n",
    "        start_date_training_fold = start_date_training - delta\n",
    "        \n",
    "        # Get the training and test (assessment) sets\n",
    "        train_df, test_df = get_train_test_set(\n",
    "            data=data,\n",
    "            start_date_training=start_date_training_fold,\n",
    "            delta_train=delta_train,\n",
    "            delta_delay=delta_delay,\n",
    "            delta_test=delta_assessment)\n",
    "    \n",
    "        # Get the indices from the two sets, and add them \n",
    "        # to the list of prequential splits\n",
    "        indices_train = list(train_df.index)\n",
    "        indices_test = list(test_df.index)\n",
    "        \n",
    "        prequential_split_indices.append((indices_train, indices_test))\n",
    "    \n",
    "    return prequential_split_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### card_precision_top_k_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def card_precision_top_k_day(df_day, top_k):\n",
    "    \n",
    "    # This takes the max of the predictions AND the max of \n",
    "    # label TX_FRAUD for each CUSTOMER_ID, \n",
    "    # and sorts by decreasing order of fraudulent prediction\n",
    "    df_day = (df_day\n",
    "              .groupby('customer_id')\n",
    "              .max()\n",
    "              .sort_values(by=\"predictions\", ascending=False)\n",
    "              .reset_index(drop=False))\n",
    "    \n",
    "    # get the top k most suspicious cards to df\n",
    "    df_day_top_k = df_day.head(top_k)\n",
    "    # get defrauded cards (customer_ids) from this df\n",
    "    loc = df_day_top_k['tx_fraud']==1, 'customer_id'\n",
    "    detected_compromised_cards_series = df_day_top_k.loc[loc]\n",
    "    # create list of them\n",
    "    list_detected_compromised_cards = list(detected_compromised_cards_series)\n",
    "    \n",
    "    # Compute precision top k\n",
    "    card_precision_top_k = len(list_detected_compromised_cards) / top_k\n",
    "    \n",
    "    return list_detected_compromised_cards, card_precision_top_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### card_precision_top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def card_precision_top_k(\n",
    "        data, top_k, remove_detected_compromised_cards=True):\n",
    "\n",
    "    # Sort days by increasing order\n",
    "    list_days=list(data['tx_time_days'].unique())\n",
    "    list_days.sort()\n",
    "    \n",
    "    # At first, the list of detected compromised cards is empty\n",
    "    list_detected_compromised_cards = []\n",
    "    \n",
    "    cpk_per_day_list = []\n",
    "    nb_compromised_cards_per_day = []\n",
    "    \n",
    "    # For each day, compute precision top k\n",
    "    for day in list_days:\n",
    "        \n",
    "        df_day = data[data['tx_time_days']==day]\n",
    "        df_day = df_day[['predictions', 'customer_id', 'tx_fraud']]\n",
    "        '''\n",
    "        df_day = (df_day[df_day['customer_id']\n",
    "                  .isin(list_detected_compromised_cards)==False])\n",
    "        '''\n",
    "        # create slice of df_day compromised cards\n",
    "        slice_compromised = (df_day['customer_id']\n",
    "                      .isin(list_detected_compromised_cards))\n",
    "        # remove detected compromised cards from the set of daily transactions\n",
    "        df_day = df_day[~slice_compromised]\n",
    "\n",
    "        # number of fraud cards (customer ids)\n",
    "        '''\n",
    "        num_cards = len(df_day[df_day['tx_fraud']==1]['customer_id'].unique())\n",
    "        nb_compromised_cards_per_day.append(num_cards)\n",
    "        '''\n",
    "        # fraud_ids = df_day[df_day['tx_fraud']==1]['customer_id'].unique()\n",
    "        # num_cards = len(fraud_ids)\n",
    "        \n",
    "        (detected_compromised_cards,\n",
    "         cpk) = card_precision_top_k_day(df_day, top_k)\n",
    "        \n",
    "        cpk_per_day_list.append(cpk)\n",
    "        \n",
    "        # Let us update the list of detected compromised cards\n",
    "        if remove_detected_compromised_cards:\n",
    "            list_detected_compromised_cards.extend(detected_compromised_cards)\n",
    "        \n",
    "    # Compute the mean\n",
    "    mean_cpk = np.array(cpk_per_day_list).mean()\n",
    "    \n",
    "    # Returns precision top k per day as a list, and resulting mean\n",
    "    return nb_compromised_cards_per_day, cpk_per_day_list, mean_cpk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### card_precision_top_k_custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def card_precision_top_k_custom(y_true, y_pred, top_k, data):\n",
    "    \n",
    "    # Let us create a predictions_df DataFrame, \n",
    "    # that contains all transactions matching the indices of \n",
    "    # the current fold (indices of the y_true vector)\n",
    "    predictions_df = data.iloc[y_true.index.values].copy()\n",
    "    predictions_df['predictions'] = y_pred\n",
    "    \n",
    "    # Compute the CP@k \n",
    "    (nb_compromised_cards_per_day,\n",
    "     cpk_per_day_list,\n",
    "     mean_cpk) = card_precision_top_k(predictions_df, top_k)\n",
    "    \n",
    "    # Return the mean_card_precision_top_k\n",
    "    return mean_cpk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prequential_grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prequential_grid_search(\n",
    "        data, estimator, param_grid, features, target, scoring,\n",
    "        start_date_training, subset='Test', preprop_list=[],\n",
    "        n_folds=4, delta_train=7, delta_delay=7, delta_assessment=7,\n",
    "        performance_metrics_list_grid=['roc_auc'],\n",
    "        performance_metrics_list=['AUC ROC'],\n",
    "        search_type='grid', n_iter=None, random_state=42, n_jobs=-1):\n",
    "\n",
    "    steps = preprop_list.copy()\n",
    "    steps.extend([('estimator', estimator)])\n",
    "                                \n",
    "    pipe = Pipeline(steps)\n",
    "    \n",
    "    prequential_split_indices = prequentialSplit(\n",
    "        data=data,\n",
    "        start_date_training=start_date_training,\n",
    "        n_folds=n_folds, \n",
    "        delta_train=delta_train, \n",
    "        delta_delay=delta_delay, \n",
    "        delta_assessment=delta_assessment\n",
    "    )\n",
    "    \n",
    "    if search_type==\"grid\":\n",
    "\n",
    "        cv = GridSearchCV(\n",
    "            estimator=pipe,\n",
    "            param_grid=param_grid,\n",
    "            scoring=scoring,\n",
    "            cv=prequential_split_indices,\n",
    "            refit=False,\n",
    "            n_jobs=n_jobs\n",
    "        )\n",
    "        \n",
    "    if search_type==\"random\":\n",
    "        \n",
    "        cv = RandomizedSearchCV(\n",
    "            estimator=pipe,\n",
    "            param_distributions=param_grid,\n",
    "            scoring=scoring,\n",
    "            cv=prequential_split_indices,\n",
    "            refit=False,\n",
    "            n_jobs=n_jobs,\n",
    "            n_iter=n_iter,\n",
    "            random_state=random_state\n",
    "        )\n",
    "        \n",
    "    X = data[features]\n",
    "    y = data[target]\n",
    "    cv.fit(X, y)\n",
    "\n",
    "    performances = pd.DataFrame()\n",
    "    \n",
    "    for i in range(len(performance_metrics_list_grid)):\n",
    "\n",
    "        metric_name = performance_metrics_list[i] + ' ' + subset\n",
    "        metric_std_name = performance_metrics_list[i] + ' ' + subset + ' Std'\n",
    "        \n",
    "        metric_grid_name = 'mean_test_' + performance_metrics_list_grid[i]\n",
    "        metric_grid_std_name = 'std_test_' + performance_metrics_list_grid[i]\n",
    "        \n",
    "        performances[metric_name] = cv.cv_results_[metric_grid_name]\n",
    "        performances[metric_std_name] = cv.cv_results_[metric_grid_std_name]\n",
    "\n",
    "    performances['Parameters'] = cv.cv_results_['params']\n",
    "    performances['Fit time'] = cv.cv_results_['mean_fit_time']\n",
    "    performances['Score time'] = cv.cv_results_['mean_score_time']\n",
    "    \n",
    "    return performances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grid_create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_create(\n",
    "        estimator, params, preprop_list, search_type='grid', n_iter=None,\n",
    "        random_state=42, key_params_idxs=0, time_exec=False):\n",
    "    \n",
    "    grid = {\n",
    "        'estimator': estimator,\n",
    "        'params': params,\n",
    "        'features': features,\n",
    "        'target': target,\n",
    "        'scoring': scoring,\n",
    "        'train_start_valid': train_start_valid,\n",
    "        'train_start_test': train_start_test,\n",
    "        'preprop_list': preprop_list,\n",
    "        'n_folds': n_folds,\n",
    "        'delta_train': delta_train,\n",
    "        'delta_delay': delta_delay,\n",
    "        'delta_assessment': delta_assessment,\n",
    "        'metrics_list_grid': metrics_grid,\n",
    "        'metrics_list': metrics,\n",
    "        'search_type': search_type,\n",
    "        'n_jobs': n_jobs,\n",
    "        'n_iter': n_iter,\n",
    "        'random_state': random_state,\n",
    "        'key_params_idxs': key_params_idxs,\n",
    "        'time_exec': time_exec\n",
    "    }\n",
    "\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model_performance_CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_performance_CV(\n",
    "        data, estimator, params, features, target,\n",
    "        preprop_list, scoring, train_start_valid, train_start_test,\n",
    "        n_folds, delta_train, delta_delay, delta_assessment,\n",
    "        metrics_list_grid, metrics_list, search_type, n_jobs,\n",
    "        n_iter=None, random_state=42, key_params_idxs=0, time_exec=True):\n",
    "\n",
    "    t_start = time.time()\n",
    "    \n",
    "    performance_validation = prequential_grid_search(\n",
    "        data=data,\n",
    "        estimator=estimator,\n",
    "        param_grid=params,\n",
    "        features=features,\n",
    "        target=target,\n",
    "        scoring=scoring,\n",
    "        start_date_training=train_start_valid,\n",
    "        preprop_list=preprop_list,\n",
    "        n_folds=n_folds,\n",
    "        subset='Validation',\n",
    "        delta_train=delta_train,\n",
    "        delta_delay=delta_delay,\n",
    "        delta_assessment=delta_assessment,\n",
    "        performance_metrics_list_grid=metrics_list_grid,\n",
    "        performance_metrics_list=metrics_list,\n",
    "        search_type=search_type,\n",
    "        n_iter=n_iter,\n",
    "        random_state=random_state,\n",
    "        n_jobs=n_jobs)\n",
    "\n",
    "\n",
    "    performance_test = prequential_grid_search(\n",
    "        data=data,\n",
    "        estimator=estimator,\n",
    "        param_grid=params,\n",
    "        features=features,\n",
    "        target=target,\n",
    "        scoring=scoring,\n",
    "        start_date_training=train_start_test,\n",
    "        preprop_list=preprop_list,\n",
    "        n_folds=n_folds,\n",
    "        subset='Test',\n",
    "        delta_train=delta_train,\n",
    "        delta_delay=delta_delay,\n",
    "        delta_assessment=delta_assessment,\n",
    "        performance_metrics_list_grid=metrics_list_grid,\n",
    "        performance_metrics_list=metrics_list,\n",
    "        search_type=search_type,\n",
    "        n_iter=n_iter,\n",
    "        random_state=random_state,\n",
    "        n_jobs=n_jobs)\n",
    "\n",
    "    cols_drop = ['Parameters', 'Fit time', 'Score time']\n",
    "    performance_validation.drop(columns=cols_drop, inplace=True)\n",
    "\n",
    "    performance = pd.concat(\n",
    "        [performance_test, performance_validation], axis=1)\n",
    "\n",
    "    summary_series = performance['Parameters']\n",
    "    params_keys_list = list(params.keys())\n",
    "    summary_params = [params_keys_list[i] for i in key_params_idxs]\n",
    "    \n",
    "    summary_list = []\n",
    "\n",
    "    for params_value in summary_series:\n",
    "        \n",
    "        summary_value_list = [params_value[i] for i in summary_params]\n",
    "        summary_value = '/'.join(map(str, summary_value_list))\n",
    "        summary_list.append(summary_value)\n",
    "        \n",
    "    performance['Parameters summary'] = summary_list\n",
    "    \n",
    "    if time_exec:\n",
    "        e_time = time.time() - t_start\n",
    "        e_time_format = str(dt.timedelta(seconds=np.round(e_time)))\n",
    "        print('Execution time: {}'.format(e_time_format))\n",
    "\n",
    "    return performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_summary_performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary_performances(performances, metrics):\n",
    "\n",
    "    df = performances.copy()\n",
    "    metrics_test = [(metric+' Test') for metric in metrics]\n",
    "\n",
    "    summary = pd.DataFrame(columns=metrics)\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    best_estimated_parameters = []\n",
    "    validation_performance = []\n",
    "    test_performance = []\n",
    "    \n",
    "    for metric in metrics:\n",
    "\n",
    "        metric_valid_name = metric + ' Validation'\n",
    "        metric_valid_name_std = metric_valid_name + ' Std'\n",
    "        metric_test_name = metric + ' Test'\n",
    "        metric_test_name_std = metric_test_name + ' Std'\n",
    "\n",
    "        metric_max = np.argmax(df[metric_valid_name].values)\n",
    "        idx_best_valid = df.index[metric_max]\n",
    "        metric_params = df['Parameters summary'].iloc[idx_best_valid]\n",
    "        \n",
    "        best_estimated_parameters.append(metric_params)\n",
    "\n",
    "        perf_valid = (df[metric_valid_name].iloc[idx_best_valid])\n",
    "        perf_valid_std = (df[metric_valid_name_std].iloc[idx_best_valid])\n",
    "        perf_test = (df[metric_test_name].iloc[idx_best_valid])\n",
    "        perf_test_std = (df[metric_test_name_std].iloc[idx_best_valid])\n",
    "\n",
    "        perf_valid = '%.3f' % round(perf_valid, 3)\n",
    "        perf_valid_std = '%.3f' % round(perf_valid_std, 3)\n",
    "        perf_test = '%.3f' % round(perf_test, 3)\n",
    "        perf_test_std = '%.3f' % round(perf_test_std, 3)\n",
    "\n",
    "        validation_performance.append(perf_valid + '+/-' + perf_valid_std)\n",
    "        test_performance.append(perf_test + '+/-' + perf_test_std)\n",
    "    \n",
    "    summary.loc['Best estimated parameters'] = best_estimated_parameters\n",
    "    summary.loc['Validation performance'] = validation_performance\n",
    "    summary.loc['Test performance'] = test_performance\n",
    "\n",
    "    optimal_test_performance = []\n",
    "    optimal_parameters = []\n",
    "\n",
    "    for metric in metrics_test:\n",
    "\n",
    "        metric_max_test = np.argmax(df[metric].values)\n",
    "        idx_opt_test = df.index[metric_max_test]\n",
    "        metric_params_test = (df['Parameters summary'].iloc[idx_opt_test])\n",
    "    \n",
    "        optimal_parameters.append(metric_params_test)\n",
    "\n",
    "        perf_opt_test = df[metric].iloc[idx_opt_test]\n",
    "        perf_opt_test_std = df[metric+' Std'].iloc[idx_opt_test]\n",
    "\n",
    "        perf_opt_test = '%.3f' % round(perf_opt_test, 3)\n",
    "        perf_opt_test_std = '%.3f' % round(perf_opt_test_std, 3)\n",
    "\n",
    "        performance_opt_test = perf_opt_test + '+/-' + perf_opt_test_std\n",
    "        optimal_test_performance.append(performance_opt_test)\n",
    "\n",
    "    summary.loc['Optimal parameter(s)'] = optimal_parameters\n",
    "    summary.loc['Optimal test performance'] = optimal_test_performance\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf_matrix(y_true, y_score, save=False, filename='0'):\n",
    "\n",
    "    matrix = confusion_matrix(y_true, y_score)\n",
    "\n",
    "    tp = matrix[1][1]\n",
    "    tn = matrix[0][0]\n",
    "    fp = matrix[0][1]\n",
    "    fn = matrix[1][0]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(4, 2))\n",
    "\n",
    "    ax = sns.heatmap(\n",
    "        matrix,\n",
    "        cmap=gradient,\n",
    "        vmin=0,\n",
    "        vmax=0,\n",
    "        annot_kws={'size': 12},\n",
    "        cbar_kws={'shrink': 1},\n",
    "        annot=True,\n",
    "        xticklabels=['NO', 'YES'],\n",
    "        yticklabels=['NO', 'YES'],\n",
    "        cbar=False,\n",
    "        linewidths=0.5,\n",
    "        linecolor='0.75',\n",
    "        fmt='g'\n",
    "    )\n",
    "\n",
    "    plt.xticks(size=9, rotation=0, y=-0.03)\n",
    "    plt.yticks(size=9, rotation=0, x=-0.01)\n",
    "\n",
    "    ax.set_ylabel('Actual', fontsize=9)\n",
    "    ax.set_xlabel('Predicted', fontsize=9)\n",
    "    ax.tick_params(left=True, bottom=True)\n",
    "    \n",
    "    ax.spines[:].set_visible(True)\n",
    "    ax.spines[:].set_linewidth(1)\n",
    "    ax.spines[:].set_color('0.35')\n",
    "\n",
    "    if save:\n",
    "        plt.savefig(\n",
    "            fname='img/{}.png'.format(filename),\n",
    "            bbox_inches='tight'\n",
    "        )\n",
    "        \n",
    "    plt.show()\n",
    "\n",
    "    return tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### threshold_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_range(\n",
    "        y_true, y_score_prob, lower=0.2,\n",
    "        upper=0.5, step=0.05, kind='prob'):\n",
    "\n",
    "    arr = np.arange(lower, upper, step)\n",
    "    df = pd.DataFrame(\n",
    "        columns={\n",
    "            'Threshold': float,\n",
    "            'Precision': None,\n",
    "            'Recall': None,\n",
    "            'F1-score': None,\n",
    "            'FP': int,\n",
    "            'FN': None,\n",
    "            'TP': None,\n",
    "            'TN': None\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if kind == 'prob':\n",
    "        \n",
    "        for threshold in arr:\n",
    "            \n",
    "            y_score_thresh = y_score_prob[:, 1]>=threshold\n",
    "            p = precision_score(y_true, y_score_thresh).round(2)\n",
    "            r = recall_score(y_true, y_score_thresh).round(2)\n",
    "            f1 = f1_score(y_true, y_score_thresh).round(2)\n",
    "\n",
    "            matrix = confusion_matrix(y_true, y_score_thresh)\n",
    "            fp = matrix[0][1]\n",
    "            fn = matrix[1][0]\n",
    "            tp = matrix[1][1]\n",
    "            tn = matrix[0][0]\n",
    "\n",
    "            l = [threshold, p, r, f1, fp, fn, tp, tn]\n",
    "            df.loc[len(df)] = l\n",
    "\n",
    "        for col in ['FP', 'FN', 'TP', 'TN']:\n",
    "            df[col] = df[col].astype(np.int64)\n",
    "\n",
    "        return df\n",
    "\n",
    "    if kind == 'func':\n",
    "        \n",
    "        for threshold in arr:\n",
    "            \n",
    "            y_score_thresh = y_score_prob[:, 1]>=threshold\n",
    "            p = precision_score(y_true, y_score_thresh).round(2)\n",
    "            r = recall_score(y_true, y_score_thresh).round(2)\n",
    "            f1 = f1_score(y_true, y_score_thresh).round(2)\n",
    "\n",
    "            j = round(threshold, 4)\n",
    "\n",
    "            l = [j, p, r, f1]\n",
    "            df.loc[len(df)] = l\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### results_df_create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_df_create(\n",
    "        model_list, model_performance_list, model_exec_time_list,\n",
    "        hyper_dict=None, drop_hyper=None):\n",
    "\n",
    "    final_df = pd.DataFrame(\n",
    "        columns={\n",
    "            'index': [],\n",
    "            'AUC ROC': [],\n",
    "            'AUC ROC Std': [],\n",
    "            'Average Precision': [],\n",
    "            'Average Precision Std': [],\n",
    "            'Card Precision@100': [],\n",
    "            'Card Precision@100 Std': [],\n",
    "            'Fit Time': [],\n",
    "            'Score Time': [],\n",
    "            'Tuning Time': [],\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    final_zip = zip(\n",
    "        model_list, model_performance_list, model_exec_time_list)\n",
    "\n",
    "    for name, perf, time in final_zip:\n",
    "\n",
    "        final_df_row = []\n",
    "        df_sorted = perf.sort_values('Average Precision Validation',\n",
    "                                     ascending=False)\n",
    "        \n",
    "        auc_roc = df_sorted.iloc[0]['AUC ROC Test']\n",
    "        auc_roc_std = df_sorted.iloc[0]['AUC ROC Test Std']\n",
    "        avg_precision = df_sorted.iloc[0]['Average Precision Test']\n",
    "        avg_precision_std = df_sorted.iloc[0]['Average Precision Test Std']\n",
    "        card_precision = df_sorted.iloc[0]['Card Precision@100 Test']\n",
    "        card_precision_std = df_sorted.iloc[0]['Card Precision@100 Test Std']\n",
    "        fit_time = df_sorted.iloc[0]['Fit time']\n",
    "        score_time = df_sorted.iloc[0]['Score time']\n",
    "\n",
    "        # round metrics\n",
    "        round_3 = [\n",
    "            auc_roc,\n",
    "            avg_precision,\n",
    "            card_precision\n",
    "        ]\n",
    "        \n",
    "        (auc_roc,\n",
    "         avg_precision,\n",
    "         card_precision) = [np.round(i, 3) for i in round_3]\n",
    "        \n",
    "        # round std deviations\n",
    "        round_4 = [\n",
    "            auc_roc_std,\n",
    "            avg_precision_std,\n",
    "            card_precision_std\n",
    "        ]\n",
    "        \n",
    "        (auc_roc_std,\n",
    "         avg_precision_std,\n",
    "         card_precision_std) = [np.round(i, 4) for i in round_4]\n",
    "\n",
    "        # add rows to df\n",
    "        cols = [\n",
    "            name, \n",
    "            auc_roc, auc_roc_std,\n",
    "            avg_precision, avg_precision_std,\n",
    "            card_precision, card_precision_std,\n",
    "            fit_time, score_time, time\n",
    "        ]\n",
    "        \n",
    "        for i in cols:\n",
    "            final_df_row.append(i)\n",
    "\n",
    "        final_df.loc[len(final_df)] = final_df_row\n",
    "\n",
    "        if hyper_dict:\n",
    "            \n",
    "            hyper_list = []\n",
    "            for model in hyper_dict.keys():\n",
    "                params = list(hyper_dict[model].values())\n",
    "                if drop_hyper:\n",
    "                    del params[-drop_hyper:]\n",
    "                else:\n",
    "                    pass\n",
    "                    \n",
    "                hyper_list.append(params)\n",
    "                \n",
    "                hyper_string = []\n",
    "                for params in hyper_list:\n",
    "                    params_string = '/'.join(map(str, params))\n",
    "                    hyper_string.append(params_string)\n",
    "        \n",
    "    final_df = final_df.set_index('index', drop=True)  \n",
    "    final_df.index.name = None\n",
    "\n",
    "    if hyper_dict:\n",
    "\n",
    "        hyper_list = []\n",
    "        \n",
    "        cols = ['Parameters'] + list(final_df.columns)\n",
    "        final_df['Parameters'] = hyper_string\n",
    "        final_df = final_df[cols]\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract_hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hyperparams(dict, model, kind='value'):\n",
    "\n",
    "    dict_model = dict[model].copy()\n",
    "\n",
    "    if kind == 'list':\n",
    "        \n",
    "        dict_model_keys = dict_model.keys()\n",
    "        \n",
    "        for key in dict_model_keys:\n",
    "            dict_model[key] = [dict_model[key]]\n",
    "    \n",
    "    return dict_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data_smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_smoothing(data, x_var, y_vars, k=3, smooth_num=300):\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    x = data[x_var].values\n",
    "    x_min = x.min()\n",
    "    x_max = x.max()\n",
    "    \n",
    "    x_smooth = np.linspace(x_min, x_max, smooth_num)\n",
    "    df[x_var] = x_smooth\n",
    "    \n",
    "    for col in y_vars:\n",
    "    \n",
    "        y = data[col].values\n",
    "        spl = make_interp_spline(x, y, k=k)\n",
    "        \n",
    "        y_smooth = spl(x_smooth)\n",
    "        df[col] = y_smooth\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_performance(\n",
    "        data, metrics, subsets,\n",
    "        title=None, xlabel=None, colors=None,\n",
    "        rotation=None, save=False, filename='0'):\n",
    "\n",
    "    if not colors:\n",
    "        colors = palette\n",
    "\n",
    "    if not xlabel:\n",
    "        xlabel = 'Hyperparameter Value'\n",
    "    \n",
    "    ncols = len(metrics)\n",
    "        \n",
    "    fig, axs = plt.subplots(\n",
    "        nrows=1, ncols=ncols,\n",
    "        sharex=True, figsize=(11.5,2.5))\n",
    "    \n",
    "    fig.suptitle(title, weight='normal', size=11, y=1.1)\n",
    "    \n",
    "    abscissa = data['Parameters summary']\n",
    "    \n",
    "    for metric in metrics:\n",
    "    \n",
    "        ax_idx = metrics.index(metric)\n",
    "        \n",
    "        for subset in subsets:\n",
    "            \n",
    "            metric_subset = metric + ' ' + subset\n",
    "            metric_subset_std = metric_subset + ' Std'\n",
    "            \n",
    "            subset_idx = subsets.index(subset)\n",
    "\n",
    "            # t-student = 3.18\n",
    "            conf_min = (data[metric_subset]\n",
    "                        - 3.18*(data[metric_subset_std]/4**0.5))\n",
    "            conf_max = (data[metric_subset]\n",
    "                        + 3.18*(data[metric_subset_std]/4**0.5))\n",
    "    \n",
    "            legend, label = (None, ) * 2\n",
    "            \n",
    "            if ax_idx == ncols-1:\n",
    "                legend = 'brief'\n",
    "                label = subset\n",
    "            try:\n",
    "                sns.lineplot(\n",
    "                    data=data,\n",
    "                    x=abscissa,\n",
    "                    y=metric_subset,\n",
    "                    legend=legend,\n",
    "                    label=label,\n",
    "                    color=colors[subset_idx],\n",
    "                    ax=axs[ax_idx]\n",
    "                );\n",
    "            except TypeError:\n",
    "                print('Specify colors')\n",
    "                plt.close()\n",
    "                return\n",
    "            \n",
    "            axs[ax_idx].fill_between(\n",
    "                abscissa,\n",
    "                conf_min,\n",
    "                conf_max,\n",
    "                color=colors[subset_idx],\n",
    "                alpha=0.07\n",
    "            )\n",
    "            \n",
    "        # optimum - parameters for max value of each metric due Validation\n",
    "        metric_name = metric + ' ' + 'Validation'\n",
    "        best_index = data[metric_name].idxmax()\n",
    "        \n",
    "        best_parameter = data.loc[best_index, 'Parameters summary']\n",
    "        best_performance = data.loc[best_index, metric_name]\n",
    "    \n",
    "        ymin = axs[ax_idx].get_ylim()[0]\n",
    "        \n",
    "        # optimum line\n",
    "        axs[ax_idx].vlines(\n",
    "            best_parameter,\n",
    "            ymin,\n",
    "            best_performance,\n",
    "            linestyles='--',\n",
    "            color=palette[-1]\n",
    "        )\n",
    "        \n",
    "        axs[ax_idx].set_title(\n",
    "            metric,\n",
    "            weight='bold',\n",
    "            size=9, y=1.03,\n",
    "            loc='center'\n",
    "        )\n",
    "        \n",
    "        axs[ax_idx].set_xlabel(\n",
    "            xlabel=xlabel,\n",
    "            weight='normal',\n",
    "            style='italic',\n",
    "            labelpad=10\n",
    "        )\n",
    "        \n",
    "        axs[ax_idx].set_ylabel(None)\n",
    "\n",
    "        if rotation:\n",
    "            axs[ax_idx].tick_params(axis='x', rotation=rotation)\n",
    "            anchor = (-0.33, -0.5)\n",
    "        else:\n",
    "            anchor = (-0.33, -0.32)\n",
    "\n",
    "    plt.legend(\n",
    "        labels=None, bbox_to_anchor=anchor,\n",
    "        ncols=2, prop={'size': 9})\n",
    "        \n",
    "    plt.subplots_adjust(wspace=0.2)\n",
    "\n",
    "    if save:\n",
    "        plt.savefig(\n",
    "            fname='img/{}.png'.format(filename),\n",
    "            bbox_inches='tight'\n",
    "        )\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(\n",
    "        data, metrics, models_list, palette,\n",
    "        limits_list, save=False, filename='0'):\n",
    "\n",
    "    width=0.55\n",
    "    fig_width = 11\n",
    "    fig_height = 3\n",
    "    t = 3.18\n",
    "    n_folds = 4\n",
    "\n",
    "    fig, axs = plt.subplots(\n",
    "        nrows=1, ncols=3, figsize=(fig_width, fig_height))\n",
    "\n",
    "    for metric in metrics:\n",
    "\n",
    "        metric_index = metrics.index(metric)\n",
    "        \n",
    "        sns.barplot(\n",
    "            data=data,\n",
    "            x=results.index,\n",
    "            y=metric,\n",
    "            width=width,\n",
    "            palette=palette,\n",
    "            ax=axs[metric_index]\n",
    "        );\n",
    "\n",
    "        ymin = limits_list[metric_index][0]\n",
    "        ymax = limits_list[metric_index][1]\n",
    "        step = limits_list[metric_index][2]\n",
    "\n",
    "        axs[metric_index].tick_params(\n",
    "            axis='x',\n",
    "            which='both',\n",
    "            bottom=False,\n",
    "            top=False,\n",
    "            labelbottom=False)\n",
    "\n",
    "        ticks=np.arange(ymin, ymax, step)\n",
    "        \n",
    "        axs[metric_index].set_ylim(ymin, ymax)\n",
    "        axs[metric_index].set_yticks(ticks=ticks)\n",
    "        axs[metric_index].yaxis.set_tick_params(labelsize=8)\n",
    "        axs[metric_index].set_title(metric, size=9)\n",
    "        axs[metric_index].set_ylabel(None)\n",
    "\n",
    "        for model in data.index:\n",
    "\n",
    "            mean = data.loc[model, metric]\n",
    "            \n",
    "            metric_std_col = metric + ' Std'\n",
    "            std = data.loc[model, metric_std_col]\n",
    "            \n",
    "            ymin = mean - t*(std/n_folds**0.5)\n",
    "            ymax = mean + t*(std/n_folds**0.5)\n",
    "            \n",
    "            axs[metric_index].plot([model, model], [ymin, ymax],\n",
    "                     color=palette[-1],\n",
    "                     linestyle='-',\n",
    "                     linewidth=1.5)\n",
    "    \n",
    "    lr = mlines.Line2D(\n",
    "        [], [], color=colors[0], marker='s', \n",
    "        linestyle='None', markersize=5, label=models_list[0])\n",
    "\n",
    "    rf = mlines.Line2D(\n",
    "        [], [], color=colors[1], marker='s',\n",
    "        linestyle='None', markersize=5, label=models_list[1])\n",
    "\n",
    "    xgb = mlines.Line2D(\n",
    "        [], [], color=colors[2], marker='s',\n",
    "        linestyle='None', markersize=5, label=models_list[2])\n",
    "\n",
    "    lgb = mlines.Line2D(\n",
    "        [], [], color=colors[3], marker='s',\n",
    "        linestyle='None', markersize=5, label=models_list[3])\n",
    "    \n",
    "    plt.legend(\n",
    "        labels=None, handles=[lr,rf,xgb,lgb], \n",
    "        loc='lower center', bbox_to_anchor=(-0.8, -0.35),\n",
    "        ncols=2, prop={'size': 9}, labelcolor='0.3'\n",
    "    )\n",
    "    \n",
    "    plt.subplots_adjust(wspace=0.3)\n",
    "    \n",
    "    if save:\n",
    "        plt.savefig(\n",
    "            fname='img/{}.png'.format(filename),\n",
    "            bbox_inches='tight'\n",
    "        )\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot_roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(\n",
    "        y_true, y_score_prob_list, name_list,\n",
    "        palette, save=True, filename='0'):\n",
    "\n",
    "    markers = ['o', 'v', 's', 'D']\n",
    "    vars_zip = zip(y_score_prob_list, name_list, palette, markers)\n",
    "    \n",
    "    plt.figure(figsize=(8, 4))\n",
    "    \n",
    "    for y_score_prob, name, color, marker in vars_zip:\n",
    "\n",
    "        fpr, tpr, thresholds = roc_curve(y_true, y_score_prob)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        label = '{0} (AUC = {1:.3f})'.format(name, roc_auc)\n",
    "\n",
    "        # roc_curve\n",
    "        plt.plot(fpr, tpr, label=label, color=color)\n",
    "\n",
    "        # random model curve\n",
    "        plt.plot([0, 1], [0, 1],\n",
    "                 color=palette[-1],\n",
    "                 linestyle='--',\n",
    "                 linewidth=0.6)\n",
    "        \n",
    "        plt.xlabel(\n",
    "            xlabel='False Positive Rate (1-Specifity)',\n",
    "            fontsize=10, weight='normal', color='0.3'\n",
    "        )\n",
    "        \n",
    "        plt.ylabel(\n",
    "            ylabel='True Positive Rate (Recall)',\n",
    "            fontsize=10, weight='normal', color='0.3'\n",
    "        )\n",
    "\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.08])\n",
    "        plt.xticks(size=9)\n",
    "        plt.yticks(size=9)\n",
    "\n",
    "        # find idx of item in thresholds closest to 0.5\n",
    "        delta = thresholds - 0.5\n",
    "        default_thresh_idx = np.argmin(np.abs(delta))\n",
    "        \n",
    "        sns.scatterplot(\n",
    "            x=[fpr[default_thresh_idx]],\n",
    "            y=[tpr[default_thresh_idx]],\n",
    "            marker=marker,\n",
    "            s=50,\n",
    "            color=palette[-1],\n",
    "            edgecolor=palette[-1],\n",
    "            linewidth=1.5,\n",
    "            facecolor='None',\n",
    "            label='0.5 Predict Probability Threshold'\n",
    "        )\n",
    "    \n",
    "    plt.legend(\n",
    "        labels=None, loc='upper right', bbox_to_anchor=(1.435, 1),\n",
    "        prop={'size': 9}, labelcolor='0.3'\n",
    "    )\n",
    "\n",
    "    if save:\n",
    "        plt.savefig(\n",
    "            fname='img/{}.png'.format(filename),\n",
    "            bbox_inches='tight'\n",
    "        )\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot_prec_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prec_rec(\n",
    "        y_true, y_score_prob_list, name_list,\n",
    "        palette, save=False, filename='0'):\n",
    "\n",
    "    metrics_opt = []\n",
    "    markers = ['o', 'v', 's', 'D']\n",
    "    vars_zip = zip(y_score_prob_list, name_list, palette, markers)\n",
    "    \n",
    "    plt.figure(figsize=(8, 4))\n",
    "    \n",
    "    for y_score_prob, name, color, marker in vars_zip:\n",
    "    \n",
    "        precision, recall, thresholds = \\\n",
    "            precision_recall_curve(y_true, y_score_prob)\n",
    "        # calculate f_score\n",
    "        f_score = (2 * precision * recall) / (precision + recall)\n",
    "        # locate index of the largest f_score\n",
    "        idx_opt = np.argmax(f_score)\n",
    "        # calculate optimum recall, precision, threshold\n",
    "        precision_opt = round(precision[idx_opt], 2)\n",
    "        recall_opt = round(recall[idx_opt], 2)\n",
    "        threshold_opt = round(thresholds[idx_opt], 2)\n",
    "\n",
    "        ap = average_precision_score(y_true, y_score_prob)\n",
    "\n",
    "        metrics_list = [precision_opt, recall_opt, threshold_opt]\n",
    "        metrics_opt.append(metrics_list)\n",
    "\n",
    "        # precision-recall curve\n",
    "        plt.plot(\n",
    "            recall, precision,\n",
    "            label='{0} (AP = {1:.3f})'.format(name, ap),\n",
    "            color=color\n",
    "        )\n",
    "\n",
    "        # optimum point\n",
    "        sns.scatterplot(\n",
    "            x=[recall_opt],\n",
    "            y=[precision_opt],\n",
    "            marker=marker,\n",
    "            s=50,\n",
    "            color=palette[-1],\n",
    "            edgecolor=palette[-1],\n",
    "            linewidth=1.5,\n",
    "            facecolor='None',\n",
    "            label='Optimal Threshold'\n",
    "        )\n",
    "\n",
    "        plt.xlabel(\n",
    "            xlabel='Recall', fontsize=10,\n",
    "            weight='normal', color='0.3')\n",
    "        \n",
    "        plt.ylabel(\n",
    "            ylabel='Precision', fontsize=10,\n",
    "            weight='normal', color='0.3')\n",
    "\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.08])\n",
    "    plt.xticks(size=9)\n",
    "    plt.yticks(size=9)\n",
    "    \n",
    "    plt.legend(\n",
    "        labels=None, loc='upper right', bbox_to_anchor=(1.425, 1),\n",
    "        prop={'size': 9}, labelcolor='0.3'\n",
    "    )\n",
    "    \n",
    "    if save:\n",
    "        plt.savefig(\n",
    "            fname='img/{}.png'.format(filename),\n",
    "            bbox_inches='tight'\n",
    "        )\n",
    "        \n",
    "    plt.show()\n",
    "\n",
    "    return metrics_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_times(\n",
    "        data, times, palette, figsize=(8,6),\n",
    "        save=False, filename='0'):\n",
    "\n",
    "    nrows = len(times)\n",
    "    \n",
    "    fig, axs = plt.subplots(\n",
    "        nrows=nrows, ncols=1, figsize=figsize)\n",
    "\n",
    "    for time in times:\n",
    "\n",
    "        time_index = times.index(time)\n",
    "\n",
    "        if len(times) == 1:\n",
    "            ax=axs\n",
    "        else:\n",
    "            ax=axs[time_index]\n",
    "\n",
    "        sns.barplot(\n",
    "            data=data,\n",
    "            x=time,\n",
    "            y=data.index,\n",
    "            palette=palette,\n",
    "            width=0.6,\n",
    "            ax=ax\n",
    "        )\n",
    "        \n",
    "        ax.set_xlabel(None)\n",
    "        ax.set_title(\n",
    "            label='{0} (sec)'.format(time),\n",
    "            weight='bold', size=9)\n",
    "        \n",
    "    plt.subplots_adjust(hspace=0.6)\n",
    "\n",
    "    if save:\n",
    "        plt.savefig(\n",
    "            fname='img/{}.png'.format(filename),\n",
    "            bbox_inches='tight'\n",
    "        )\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot_unbalanced_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_unbalanced_model(\n",
    "        data, metrics_list, colors,\n",
    "        n_folds, limits_list, save=False, filename='0'):\n",
    "\n",
    "    linewidth = 1.5\n",
    "    alpha = 0.25\n",
    "    t = 3.18\n",
    "    \n",
    "    plt.figure(figsize=(11.5,5))\n",
    "\n",
    "    rows = int(np.ceil(len(metrics_list) / 2))\n",
    "    cols = 2\n",
    "\n",
    "    for metric in metrics_list:\n",
    "\n",
    "        metric_index = metrics_list.index(metric)\n",
    "        \n",
    "        plt.subplot(rows,cols,metric_index+1)\n",
    "        plt.title(metric, size=9)\n",
    "        \n",
    "        sns.lineplot(\n",
    "            data=data,\n",
    "            x=data.index,\n",
    "            y=metric,\n",
    "            linewidth=linewidth,\n",
    "            alpha=alpha,\n",
    "            color=colors[metric_index]\n",
    "        );\n",
    "\n",
    "        sns.scatterplot(\n",
    "            data=data,\n",
    "            x=data.index,\n",
    "            y=metric,\n",
    "            s=25,\n",
    "            alpha=1,\n",
    "            color=colors[metric_index]\n",
    "        )\n",
    "\n",
    "        column_std_list = list(data.columns)\n",
    "        metric_std = metric + ' Std'\n",
    "        \n",
    "        if metric_std in column_std_list:\n",
    "        \n",
    "            for model in data.index:\n",
    "    \n",
    "                metric_std = metric + ' Std'\n",
    "                \n",
    "                mean = data.loc[model, metric]\n",
    "                std = data.loc[model, metric_std]\n",
    "                \n",
    "                ymin = mean - t*(std/n_folds**0.5)\n",
    "                ymax = mean + t*(std/n_folds**0.5)\n",
    "        \n",
    "                # plot confidence intervals\n",
    "                plt.plot([model, model],\n",
    "                         [ymin, ymax],\n",
    "                          color=colors[metric_index],\n",
    "                          linewidth=1.4,\n",
    "                          alpha=0.75)\n",
    "        \n",
    "        ymin = np.arange(limits_list[metric_index][0],\n",
    "                         limits_list[metric_index][1],\n",
    "                         limits_list[metric_index][2])[0]\n",
    "        ymax = np.arange(limits_list[metric_index][0],\n",
    "                         limits_list[metric_index][1],\n",
    "                         limits_list[metric_index][2])[-1]\n",
    "            \n",
    "        plt.ylim(ymin, ymax)\n",
    "        plt.yticks(\n",
    "            np.arange(limits_list[metric_index][0],\n",
    "                      limits_list[metric_index][1],\n",
    "                      limits_list[metric_index][2])\n",
    "        )\n",
    "        plt.ylabel(None)\n",
    "        \n",
    "    plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
    "\n",
    "    if save:\n",
    "        plt.savefig(\n",
    "            fname='img/{}.png'.format(filename),\n",
    "            bbox_inches='tight'\n",
    "        )\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot_unbalanced_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_unbalanced_metric(\n",
    "        data, metric, colors, models_list,\n",
    "        save=False, filename='0'):\n",
    "\n",
    "    plt.figure(figsize=(8,4))\n",
    "    \n",
    "    sns.lineplot(\n",
    "        data=data,\n",
    "        x=data.index,\n",
    "        y=metric,\n",
    "        hue='model',\n",
    "        legend=False,\n",
    "        palette=colors,\n",
    "        alpha=0.25,\n",
    "        linewidth=1.5\n",
    "    )\n",
    "    \n",
    "    sns.scatterplot(\n",
    "        data=data,\n",
    "        x=data.index,\n",
    "        y=metric,\n",
    "        hue='model',\n",
    "        palette=colors,\n",
    "        marker='s',\n",
    "        s=40,\n",
    "        alpha=1\n",
    "    )\n",
    "\n",
    "    lr = mlines.Line2D(\n",
    "        [], [], color=colors[0], marker='s',\n",
    "        linestyle='None', markersize=5, label=models_list[0])\n",
    "\n",
    "    rf = mlines.Line2D(\n",
    "        [], [], color=colors[1], marker='s',\n",
    "        linestyle='None', markersize=5, label=models_list[1])\n",
    "\n",
    "    xgb = mlines.Line2D(\n",
    "        [], [], color=colors[2], marker='s',\n",
    "        linestyle='None', markersize=5, label=models_list[2])\n",
    "\n",
    "    lgb = mlines.Line2D(\n",
    "        [], [], color=colors[3], marker='s',\n",
    "        linestyle='None', markersize=5, label=models_list[3])\n",
    "    \n",
    "    plt.legend(\n",
    "        labels=None, handles=[lr,rf,xgb,lgb], \n",
    "        loc='upper right', bbox_to_anchor=(1.30, 1.025),\n",
    "        prop={'size': 9}, labelcolor='0.3' \n",
    "    )\n",
    "\n",
    "    plt.ylabel(metric, weight='normal', size=10)\n",
    "\n",
    "    if save:\n",
    "        plt.savefig(\n",
    "            fname='img/{}.png'.format(filename),\n",
    "            bbox_inches='tight'\n",
    "        )\n",
    "\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
